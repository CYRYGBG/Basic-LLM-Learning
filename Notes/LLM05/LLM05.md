

## 参考链接

[Preference Tuning LLMs with Direct Preference Optimization Methods](https://huggingface.co/blog/pref-tuning)

[数据集格式](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/data_preparation.html#id10)

[知乎问答偏好数据集](https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k)	

[综述：A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More](https://arxiv.org/abs/2407.16216)

[深度对比: SFT、ReFT、RHLF、RLAIF、DPO、PPO](https://www.cnblogs.com/Microsoftdeveloper/articles/18635866)



## PPO(近端策略优化)

**原文链接：**[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)





## DPO(直接偏好优化)

**原文链接：**[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)





## IPO(身份偏好优化)

原文链接：





## KTO(卡尼曼-特沃斯基优化)

原文链接：



