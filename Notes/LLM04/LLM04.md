> 希望尽可能将自己学习过程中参考过的资料进行系统的整理，方便后面的初学者更快的找到自己想要的资料！

**笔记持续更新中......**

[LLM基础学习01：LLM解码策略和显存占用计算](https://zhuanlan.zhihu.com/p/21348048780)

[LLM基础学习02：分布式训练核心架构与多级并行策略详解——DDP/FSDP/ZeRO实战代码、显存优化方案及技术资源全景索引](https://zhuanlan.zhihu.com/p/21784954155)

[LLM基础学习03：Qwen2.5-1.5B-Instruct指令微调全流程实践——LLaMA Factory框架与GSM8K评估](https://zhuanlan.zhihu.com/p/22864281740)

**本文的所有代码都放在了仓库[Basic-LLM-Learning](https://github.com/CYRYGBG/Basic-LLM-Learning)中，欢迎star！！！**

# TODO

- [x] 测试下现在的显存计算脚本能不能跑
- [ ] 把所有模型的yaml文件放在一个文件夹里
  - [ ] 做好文件命名
  - [ ] 每个文件头标好运行命令

- [ ] 找几篇综述，看下现在主要有哪些微调算法
- [ ] 完成llama的所有算法的测试和时间计算
- [ ] 完成Swift所有算法的测试和时间计算

# 简介

不同库之间支持的微调方法不同，本文主要依据[Swift](https://swift.readthedocs.io/zh-cn/stable/Instruction/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0.html#tuner)和[LLaMA Factory](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/adapters.html#)进行学习和比较。



# Full Parameter Fine-tuning（全参微调）



# Freeze



# LoRA



# LoRA+



# rsLoRA



# DoRA



# PiSSA



# Galore



# BAdam



