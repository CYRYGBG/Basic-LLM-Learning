# ./gpu_monitor.sh 0,1 FORCE_TORCHRUN=1 llamafactory-cli /root/Basic-LLM-Learning/Code/LLM03/train_yaml/test.yaml

# ------------------- 基础模型配置 -------------------
model_name_or_path: /root/model/Qwen2.5-1.5B  # 使用Qwen2.5-1.5B基座模型

# ------------------- 训练阶段配置 -------------------
stage: sft
do_train: true
finetuning_type: full  #full


# ------------------- 数据集配置 -------------------
dataset_dir: /root/dataset/openai___gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
dataset: gsm8k_math_train               # 对应JSON中定义的数据集名称
max_samples: 1000  
template: qwen  
cutoff_len: 1024  
overwrite_cache: true  
preprocessing_num_workers: 8  

# ------------------- 训练输出相关 -------------------
output_dir: /root/Basic-LLM-Learning/Code/LLM03/output/test  
logging_steps: 10  # 每10步输出一次日志
save_steps: 100  # 每100步保存一次检查点
plot_loss: true  # 绘制训练损失曲线

# ------------------- 训练超参数 -------------------
per_device_train_batch_size: 32  
gradient_accumulation_steps: 4              
learning_rate: 3.0e-5                       
num_train_epochs: 1                        
max_grad_norm: 0.5                          
lr_scheduler_type: cosine                   
warmup_ratio: 0.15                          
weight_decay: 0.05                          

# ------------------- 验证与评估 -------------------
val_size: 0.1  # 10%数据作为验证集
per_device_eval_batch_size: 4  
eval_strategy: steps 
eval_steps: 100  

# ------------------- 显存优化 -------------------
gradient_checkpointing: true  
optim: adamw_torch  

deepspeed: /root/Basic-LLM-Learning/Code/LLM03/LLaMA-Factory/examples/deepspeed/ds_z3_config.json
