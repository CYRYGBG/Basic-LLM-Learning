{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "保存文件结构验证:\n",
      "config.json: ✅\n",
      "model.safetensors: ✅\n",
      "tokenizer_config.json: ✅\n",
      "special_tokens_map.json: ✅\n",
      "tokenizer.model: ❌\n",
      "\n",
      "模型类型配置: qwen2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def save_merged_model(model, tokenizer, output_dir):\n",
    "    \"\"\"合并LoRA权重并保存完整模型\"\"\"\n",
    "    # 合并LoRA适配器到基础模型\n",
    "    if isinstance(model, PeftModel):\n",
    "        merged_model = model.merge_and_unload()\n",
    "    else:\n",
    "        merged_model = model\n",
    "    \n",
    "    # 确保模型类型与原始配置一致\n",
    "    original_config = merged_model.config.to_dict()\n",
    "    if \"model_type\" not in original_config:\n",
    "        original_config[\"model_type\"] = \"qwen2\"  # 根据实际模型修改\n",
    "    \n",
    "    # 保存完整模型\n",
    "    merged_model.save_pretrained(\n",
    "        output_dir,\n",
    "        safe_serialization=True,\n",
    "        max_shard_size=\"5GB\"  # 自动分片\n",
    "    )\n",
    "    \n",
    "    # 显式保存修复后的配置文件\n",
    "    with open(f\"{output_dir}/config.json\", \"w\") as f:\n",
    "        json.dump(original_config, f, indent=4)\n",
    "    \n",
    "    # 保存分词器\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # 验证保存结果\n",
    "    verify_saved_model(output_dir)\n",
    "\n",
    "def verify_saved_model(model_path):\n",
    "    \"\"\"验证保存的模型完整性\"\"\"\n",
    "    required_files = [\n",
    "        \"config.json\",\n",
    "        \"model.safetensors\",  # 或 pytorch_model.bin\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"tokenizer.model\"  # 针对某些分词器的特殊文件\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n保存文件结构验证:\")\n",
    "    for file in required_files:\n",
    "        exists = (Path(model_path)/file).exists()\n",
    "        print(f\"{file}: {'✅' if exists else '❌'}\")\n",
    "    \n",
    "    # 检查模型类型配置\n",
    "    with open(f\"{model_path}/config.json\") as f:\n",
    "        config = json.load(f)\n",
    "        assert \"model_type\" in config, \"缺少model_type字段\"\n",
    "        print(f\"\\n模型类型配置: {config['model_type']}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载原始模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"./models/Qwen2.5-1.5B-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    # 加载训练后的LoRA适配器\n",
    "    trained_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        \"./grpo_finetuned_model\",\n",
    "        is_trainable=False\n",
    "    )\n",
    "    \n",
    "    # 加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"./models/Qwen2.5-1.5B-Instruct\"\n",
    "    )\n",
    "    \n",
    "    # 保存合并后的完整模型\n",
    "    save_merged_model(\n",
    "        model=trained_model,\n",
    "        tokenizer=tokenizer,\n",
    "        output_dir=\"./models/merged_grpo_model\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
