{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "kLHQHIgTTlLW",
      "metadata": {
        "id": "kLHQHIgTTlLW"
      },
      "source": [
        "<div style=\"display: flex; justify-content: center;\">\n",
        "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
        "        <table style=\"width: 100%\">\n",
        "            <tr>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <span style=\"font-size: 14px;\">\n",
        "                        This is an extension notebook for <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">The Hundred-Page Language Models Book</a> by Andriy Burkov<br><br>\n",
        "                        Code repository: <a href=\"https://github.com/aburkov/theLMbook\" target=\"_blank\" rel=\"noopener\">https://github.com/aburkov/theLMbook</a>\n",
        "                    </span>\n",
        "                </td>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
        "                        <img src=\"https://thelmbook.com/img/book.png\" width=\"80px\" alt=\"The Hundred-Page Language Models Book\">\n",
        "                    </a>\n",
        "                </td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l71IC5KUOBtB",
      "metadata": {
        "id": "l71IC5KUOBtB"
      },
      "source": [
        "# 从零开始编写GRPO：使用Qwen2.5-1.5B-Instruct实现分布式指南\n",
        "\n",
        "> 如果您购买了[这本书](https://thelmbook.com/)，您可以通过发送邮件至[author@thelmbook.com](mailto:author@thelmbook.com)向[Lambda](https://lambdalabs.com/)申请$150的免费云GPU积分。使用这些云积分，您可以启动一个8xA100实例，并按照本教程的所有步骤进行操作，包括分布式训练。\n",
        "\n",
        "在本教程中，我们将演示如何使用GRPO（Group Relative Policy Optimization）方法构建一个分布式强化学习（RL）管道，以微调一个用于数学、逻辑和编码任务的语言模型。这些任务存在唯一的正确答案，可以通过简单的字符串比较轻松地验证其与真实答案的一致性。\n",
        "\n",
        "GRPO由DeepSeek发明，并用于微调DeepSeek R1和R1-Zero模型，使其通过生成思维链（CoT）在数学和逻辑任务中表现出色。您可以在[这篇文章](https://thelmbook.com/articles/#!./DeepSeek-R1.md)中找到关于R1和R1-Zero训练的详细概述。\n",
        "\n",
        "本教程的目标是将通用语言模型**Qwen2.5-1.5B-Instruct**转变为数学问题解决器。我们将从零开始编写GRPO代码，然后将其与多个流行的库和工具集成，以实现分布式训练管道，包括：\n",
        "\n",
        "- **PyTorch:** 用于张量操作和分布式训练。\n",
        "- **Hugging Face Transformers:** 用于加载预训练的语言模型和分词器。\n",
        "- **FlashAttention2:** 用于优化注意力机制，帮助减少内存使用并提高训练速度。\n",
        "- **Weights & Biases (wandb):** 用于实验跟踪、可视化和模型版本控制。\n",
        "\n",
        "本教程分为几个部分。我们从基本设置和导入开始，然后转到数据格式化和答案提取、数据集准备、评估函数、奖励函数、训练设置和执行，最后加载和测试模型。在此过程中，我们从零开始实现GRPO算法。\n",
        "\n",
        "## 第一部分：基本设置和导入\n",
        "\n",
        "在第一部分中，我们安装并导入所有必要的模块。我们还通过配置随机种子以确保可重复性，并初始化实验跟踪所需的环境变量。此外，我们安装并导入提供优化Transformer注意力机制（FlashAttention2）和报告（Weights and Biases）的库。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4qe523FMOBtC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qe523FMOBtC",
        "outputId": "fbda3aae-1a8c-4be9-cba9-7535d3336f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-03-08 11:21:14,625] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::substr(unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_stringbuf_init(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str(std::string const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_dispose(std::allocator<char> const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::swap(std::__detail::_List_node_base&, std::__detail::_List_node_base&)@GLIBCXX_3.4.15'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_mutate(unsigned long, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_next_bkt(unsigned long) const@GLIBCXX_3.4.18'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream(std::_Ios_Openmode)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::end()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::flush<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::basic_stringstream(std::string const&, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::runtime_error(std::string const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long) const@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(std::string const&) const@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
            "/home/yeqi3/anaconda3/envs/cyr/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
            "collect2: error: ld returned 1 exit status\n",
            "2025-03-08 11:21:16.001869: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-08 11:21:16.014795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741432876.029809 1146878 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741432876.034325 1146878 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-08 11:21:16.049250: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# !pip install tf-keras # for some reason, Hugging Face cannot work without it\n",
        "# !pip install flash-attn # FlashAttention2\n",
        "# !pip install wandb # Weights and Biases\n",
        "# !pip install 'accelerate>=0.26.0'\n",
        "# !pip install transformers # Hugging Face Transformers API\n",
        "# !pip install datasets # Hugging Face Datasets API\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
        "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Import necessary libraries\n",
        "# Basic Python libraries for various operations\n",
        "import random\n",
        "import copy\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "# PyTorch and related libraries for deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import deepspeed\n",
        "\n",
        "# Hugging Face libraries for transformer models\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 使用镜像站下载\n",
        "import os\n",
        "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "\n",
        "\n",
        "def set_random_seed(seed: int = 42):\n",
        "    \"\"\"\n",
        "    Set the random seed for reproducibility across Python, NumPy, and PyTorch.\n",
        "\n",
        "    Args:\n",
        "        seed (int): The seed value to use for random number generation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Explanation:\n",
        "        1. Sets seed for Python's built-in random module for basic random operations.\n",
        "        2. Sets seed for NumPy, ensuring consistent random number generation in array operations.\n",
        "        3. Sets seed for PyTorch CPU operations.\n",
        "        4. If CUDA is available, sets seed for all GPU devices.\n",
        "        5. Configures cuDNN to ensure deterministic behavior:\n",
        "           - Sets deterministic flag to True, ensuring reproducible results.\n",
        "           - Disables benchmarking to prevent algorithm selection based on hardware.\n",
        "\n",
        "    Note:\n",
        "        Setting deterministic behavior may impact performance but ensures consistent results\n",
        "        across multiple runs, which is crucial for debugging and research.\n",
        "    \"\"\"\n",
        "    # Set the seed for Python's built-in random module\n",
        "    random.seed(seed)\n",
        "    # Set the seed for NumPy\n",
        "    np.random.seed(seed)\n",
        "    # Set the seed for PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Call the function to set random seed for reproducibility\n",
        "set_random_seed(42)\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# 不使用这一部分, 改用tensorboard\n",
        "# Set environment variables for Weights & Biases (wandb) logging\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"\"\n",
        "# os.environ[\"WANDB_PROJECT\"] = \"GRPO-Qwen-1.5-Instruct-Multi-GPU\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DEcqOtaIOBtC",
      "metadata": {
        "id": "DEcqOtaIOBtC"
      },
      "source": [
        "上述代码执行以下任务：\n",
        "- **设置随机种子：** `set_random_seed` 函数通过为Python的随机模块、NumPy和PyTorch设置种子来确保可重复性。它还配置了PyTorch的cuDNN后端以实现确定性行为。\n",
        "- **环境变量配置：** 我们设置了 `WANDB_API_KEY` 和 `WANDB_PROJECT` 环境变量，以启用Weights & Biases的实验跟踪功能。\n",
        "- **导入必要的包：** 脚本导入了管道所需的所有必要模块，包括：\n",
        "以下是对每个导入的详细说明：\n",
        "- **random：** 用于数据集洗牌和随机操作。\n",
        "- **copy：** 提供深度复制对象的功能。\n",
        "- **re：** 提供正则表达式支持以进行文本处理。\n",
        "- **numpy (np)：** 支持数值操作和数组处理。\n",
        "- **torch：** 提供GPU加速的张量操作和深度学习原语。\n",
        "- **torch.nn：** 包含神经网络模块和操作。\n",
        "- **pad_sequence：** 处理可变长度序列的填充以进行批处理。\n",
        "- **AutoTokenizer** 和 **AutoModelForCausalLM：** 加载预训练的语言模型及其分词器。\n",
        "- **load_dataset：** 从Hugging Face的datasets库加载数据集。\n",
        "\n",
        "## 第二部分：数据格式化和答案提取\n",
        "在本节中，我们定义了数据的格式化方式以及如何从模型输出和数据集中提取答案部分。为了确保\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b0Pi3LMROBtD",
      "metadata": {
        "id": "b0Pi3LMROBtD"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "And must have '####' before the final answer number.\n",
        "\"\"\"\n",
        "\n",
        "# 从模型输出中获取<answer>和</answer>之间的部分\n",
        "def extract_answer_from_model_output(text):\n",
        "   \"\"\"\n",
        "   Extracts the value from the last <answer> tag in the text.\n",
        "\n",
        "   Args:\n",
        "       text (str): The model-generated text containing XML-style <answer> tags.\n",
        "\n",
        "   Returns:\n",
        "       str or None: The content inside the <answer> tags, or None if no valid answer is found.\n",
        "\n",
        "   Explanation:\n",
        "       1. Splits the text on the <answer> tag to isolate content after the tag.\n",
        "       2. Checks if at least one <answer> tag exists in the text.\n",
        "       3. For the last <answer> segment:\n",
        "          - Verifies it contains a closing </answer> tag.\n",
        "          - Extracts only the content between the tags.\n",
        "       4. Returns None if the answer is empty (just \"...\") or if tags are missing.\n",
        "   \"\"\"\n",
        "   # Split on <answer> and take everything after the last occurrence\n",
        "   parts = text.split(\"<answer>\")\n",
        "   # 如果没有找到<answer>，那就是没有进行切割，长度只有1\n",
        "   if len(parts) < 2:  \n",
        "       return None\n",
        "   last_part = parts[-1]  # 只取最后一个<answer>部分\n",
        "\n",
        "   # 结尾没有</answer>\n",
        "   if \"</answer>\" not in last_part:\n",
        "       return None\n",
        "   answer = last_part.split(\"</answer>\")[0].strip()\n",
        "   return None if answer == \"...\" else answer\n",
        "\n",
        "# 从数据集中提取答案\n",
        "def extract_answer_from_dataset(text):\n",
        "   \"\"\"\n",
        "   Extracts the answer from the GSM8K dataset examples.\n",
        "\n",
        "   Args:\n",
        "       text (str): The dataset example text containing a question and answer.\n",
        "\n",
        "   Returns:\n",
        "       str or None: The extracted answer part after the '####' delimiter, or None if not found.\n",
        "\n",
        "   Explanation:\n",
        "        1. 检查文本是否包含用于分隔问题和答案的 `####` 分隔符。\n",
        "        2. 如果找到，则在分隔符处分割文本并返回第二部分（即答案）。\n",
        "        3. 答案会去除首尾的空白字符。\n",
        "        4. 如果不存在分隔符，则返回 None。\n",
        "   \"\"\"\n",
        "   if \"####\" not in text:\n",
        "       return None\n",
        "   return text.split(\"####\")[1].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SN-yP0NoOBtD",
      "metadata": {
        "id": "SN-yP0NoOBtD"
      },
      "source": [
        "在上述代码中：\n",
        "\n",
        "- **`SYSTEM_PROMPT`：** 该字符串变量指示模型在 `<reasoning>` 标签中生成其思维链，并在 `<answer>` 标签中生成最终答案。使用这种一致的格式使得提取和评估答案变得更加容易。\n",
        "- **`extract_answer_from_model_output`：** 该函数通过 `<answer>` 标签分割生成的文本，确保仅提取最后一次出现的标签中的内容。如果标签缺失或答案无效（例如，它是一个占位符 `\"...\"`），则该函数返回 `None`。\n",
        "- **`extract_answer_from_dataset`：** 鉴于GSM8K数据集使用分隔符（`\"####\"`）来分隔答案，该函数通过在该分隔符上分割文本来提取预期的答案。\n",
        "\n",
        "## 第三部分：数据集准备\n",
        "\n",
        "在这一部分中，我们为训练准备GSM8K数据集。GSM8K是一个包含8.5K个高质量、语言多样的小学数学文字问题的数据集，由人类问题编写者创建。我们将使用该数据集中的示例来在强化学习（RL）范式中训练我们的模型：模型将生成多个问题解决方案样本，我们将这些解决方案与GSM8K示例中的真实数字进行比较，如果匹配，我们将为RL算法（GRPO）提供高奖励，从而更新模型的权重，以提高下次获得高奖励的概率。\n",
        "\n",
        "我们首先从Hugging Face加载数据集，然后将每个示例**格式化为包含系统提示和用户提示**。我们还从数据集中提取预期的答案。这里定义了两个辅助函数：\n",
        "\n",
        "1. **`prepare_dataset`：** 通过创建一个包含系统提示（带有格式化指令）和用户消息（问题）的提示来加载和准备GSM8K数据集。它还从数据集中提取答案。\n",
        "2. **`build_prompt`：** 将消息字典列表连接为单个提示字符串。这确保了在训练和推理过程中提示构建的一致性。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "F7KVdBmVOBtD",
      "metadata": {
        "id": "F7KVdBmVOBtD"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(split=\"train\"):\n",
        "    \"\"\"\n",
        "    Load and prepare the GSM8K dataset for training with string prompts.\n",
        "\n",
        "    Args:\n",
        "        split (str): The dataset split to load (\"train\" or \"test\"). Defaults to \"train\".\n",
        "\n",
        "    Returns:\n",
        "       list: A list of formatted examples, each containing a prompt string and answer.\n",
        "\n",
        "    Explanation:\n",
        "        1. 从 Hugging Face 的 datasets hub 加载 GSM8K 数据集。\n",
        "        2. 对于数据集中的每个示例：\n",
        "        - 创建一个包含系统提示和问题的消息列表。\n",
        "        - 使用 `build_prompt()` 将此列表转换为单个字符串提示。\n",
        "        - 从数据集示例中提取答案。\n",
        "        - 创建一个包含提示和答案的格式化示例字典。\n",
        "        3. 返回准备好的格式化示例列表，用于模型训练或评估。\n",
        "    \"\"\"\n",
        "    data = load_dataset('openai/gsm8k', 'main', cache_dir=\"./dataset\", download_mode=\"reuse_dataset_if_exists\")[split]\n",
        "    formatted_data = []\n",
        "\n",
        "    tmp = data[0]\n",
        "    print(\"-----------打印示例数据----------\")\n",
        "    print(\"question: \\n\", tmp[\"question\"])\n",
        "    print(\"answer: \\n\", tmp[\"answer\"])\n",
        "\n",
        "    for example in data:\n",
        "        # 将消息列表转换为单个字符串\n",
        "        prompt_str = build_prompt([\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": example[\"question\"]}\n",
        "        ])\n",
        "        formatted_example = {\n",
        "            \"prompt\": prompt_str,  # Now a string rather than a list.\n",
        "            \"answer\": extract_answer_from_dataset(example[\"answer\"])\n",
        "        }\n",
        "        formatted_data.append(formatted_example)\n",
        "    return formatted_data\n",
        "\n",
        "def build_prompt(messages):\n",
        "   \"\"\"\n",
        "   Build a single prompt string from a list of messages.\n",
        "\n",
        "   Args:\n",
        "       messages (list): A list of message dictionaries, each with 'role' and 'content' keys.\n",
        "\n",
        "   Returns:\n",
        "       str: A concatenated string of all message contents.\n",
        "\n",
        "   Explanation:\n",
        "        1. 获取一个以典型聊天格式组织的消息字典列表。\n",
        "        2. 从每条消息中提取 `content` 字段并去除空白字符。\n",
        "        3. 将所有内容字符串用换行符连接，生成一个单一的提示。\n",
        "        4. 这种方法在从结构化消息转换为字符串的同时，保留了训练格式。\n",
        "   \"\"\"\n",
        "   return \"\\n\".join([msg[\"content\"].strip() for msg in messages])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9TDvQ8SwwQW2",
      "metadata": {
        "id": "9TDvQ8SwwQW2"
      },
      "source": [
        "## 第四部分：评估函数\n",
        "\n",
        "评估对于跟踪模型的进展至关重要。在这一部分中，我们定义了一些函数，使我们能够在一组示例上评估模型。评估函数执行以下任务：\n",
        "\n",
        "- **对提示进行分词并生成响应：** 给定分词后的提示，生成模型的输出。\n",
        "- **提取预测的答案：** 从生成的响应中提取答案。\n",
        "- **将预测的答案与预期答案进行比较：** 通过精确匹配和数值等价性检查来完成这一比较。\n",
        "\n",
        "两个辅助函数 `_extract_last_number` 和 `_extract_single_number` 用于从文本中提取数字。主要的评估函数 `evaluate_model` 使用这些辅助函数来确定预测的答案是否正确：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "tC-vKpqhOBtD",
      "metadata": {
        "id": "tC-vKpqhOBtD"
      },
      "outputs": [],
      "source": [
        "# 从文本中提取最后一个数字\n",
        "def extract_last_number(text):\n",
        "   \"\"\"\n",
        "   Extracts the last number appearing in the text.\n",
        "\n",
        "   Args:\n",
        "       text (str): The text to extract a number from.\n",
        "\n",
        "   Returns:\n",
        "       float or None: The last number in the text, or None if no number is found.\n",
        "\n",
        "   Explanation:\n",
        "       1. Removes dollar signs and percent symbols from the text.\n",
        "       2. Uses regex to find a number that appears at the end of the text (possibly after whitespace).\n",
        "       3. The pattern matches numbers that appear at the end of the string, with or without decimal points.\n",
        "       4. Returns the found number as a float, or None if no match is found.\n",
        "   \"\"\"\n",
        "   # 清理文本中的特殊符号：移除美元符号($)和百分号(%)\n",
        "   text = text.replace('$', '').replace('%', '')\n",
        "   # 正则表达式模式匹配行尾的数值（支持负数、小数、空格和等号前缀）\n",
        "   pattern = r'(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$'\n",
        "   # 在文本中搜索匹配模式的内容\n",
        "   match = re.search(pattern, text)\n",
        "   # 返回提取的数值（浮点数），若未匹配则返回None\n",
        "   return float(match.group(1)) if match else None\n",
        "\n",
        "# 只有一个数字的情况下使用，其余情况返回None\n",
        "def extract_single_number(text):\n",
        "   \"\"\"\n",
        "   Extracts a single number from text if exactly one number is present.\n",
        "\n",
        "   Args:\n",
        "       text (str): The text to extract a number from.\n",
        "\n",
        "   Returns:\n",
        "       float or None: The single number in the text, or None if zero or multiple numbers are found.\n",
        "\n",
        "   Explanation:\n",
        "       1. Uses regex to find all numbers in the text (including negative numbers and decimals).\n",
        "       2. If exactly one number is found, returns it as a float.\n",
        "       3. If zero or multiple numbers are found, returns None.\n",
        "   \"\"\"\n",
        "   numbers = re.findall(r'-?\\d*\\.?\\d+', text)\n",
        "   return float(numbers[0]) if len(numbers) == 1 else None\n",
        "\n",
        "def evaluate_model(model, tokenizer, eval_examples, device):\n",
        "   \"\"\"\n",
        "   Evaluates the model on a set of examples and prints detailed results.\n",
        "\n",
        "   Args:\n",
        "        model: 待评估的语言模型。\n",
        "        tokenizer: 用于编码输入和解码输出的分词器。\n",
        "        eval_examples (list): 评估示例列表，每个示例包含 \"prompt\" 和 \"answer\"。\n",
        "        device: 运行评估的设备, CPU 或 GPU。\n",
        "\n",
        "\n",
        "   Returns:\n",
        "       float: The accuracy percentage (correct predictions / total examples * 100).\n",
        "\n",
        "   Explanation:\n",
        "        1. 将模型设置为评估模式。\n",
        "        2. 对于评估集中的每个示例：\n",
        "            - 对提示进行编码并使用模型生成响应。\n",
        "            - 从生成的响应中提取预测的答案。\n",
        "            - 使用多种方法将预测的答案与预期的答案进行比较：\n",
        "                a. 精确字符串匹配\n",
        "                b. 单数字提取和比较\n",
        "                c. 最后一个数字提取和比较\n",
        "            - 打印每个示例的详细信息。\n",
        "        3. 计算并返回总体准确率。\n",
        "        4. 将模型恢复为训练模式。\n",
        "\n",
        "   \"\"\"\n",
        "   # 将模型设置为评估模式\n",
        "   model.eval()\n",
        "   correct = 0\n",
        "   total = len(eval_examples)\n",
        "   print(\"\\n\" + \"=\"*50)\n",
        "   print(\"EVALUATION ON\", total, \"EXAMPLES\")\n",
        "   print(\"=\"*50)\n",
        "\n",
        "   for example in eval_examples:\n",
        "       # Get the prompt and expected answer\n",
        "       full_prompt = example[\"prompt\"]\n",
        "       expected = example[\"answer\"]\n",
        "\n",
        "       # Tokenize and generate response\n",
        "       inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
        "       with torch.no_grad():\n",
        "           outputs = model.generate(\n",
        "               inputs,                                      # 输入张量\n",
        "               max_new_tokens=512,                          # 最大生成 token 数量\n",
        "               temperature=0.7,                             # 控制生成文本的随机性\n",
        "               num_return_sequences=1,                      # 只生成一个输出序列\n",
        "               pad_token_id=tokenizer.pad_token_id,         # 填充 token 的 ID\n",
        "               eos_token_id=tokenizer.eos_token_id,         # 结束 token 的 ID\n",
        "               forced_eos_token_id=tokenizer.eos_token_id,  # 强制在生成结束时添加结束 token\n",
        "               early_stopping=False,                        # 不启用早停机制（生成固定长度）\n",
        "           )\n",
        "       # 将生成的 token 序列解码为可读文本\n",
        "       response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "       try:\n",
        "           # Extract answer and check correctness\n",
        "           predicted = extract_answer_from_model_output(response)\n",
        "\n",
        "           # Try different matching methods\n",
        "           if predicted == expected:  # 完全精确匹配\n",
        "               is_correct = True\n",
        "           else:\n",
        "               # 尝试提取单个数值进行匹配\n",
        "               pred_num = extract_single_number(str(predicted)) # 从模型答案中提取数值\n",
        "               exp_num = extract_single_number(str(expected))   # 从正确答案中提取数值\n",
        "               if pred_num is not None and exp_num is not None and pred_num == exp_num:\n",
        "                   is_correct = True\n",
        "               else:\n",
        "                   # 尝试提取最后一个数值进行匹配\n",
        "                   pred_num = extract_last_number(str(predicted))   # 从模型答案中提取数值    \n",
        "                   exp_num = extract_last_number(str(expected))     # 从正确答案中提取数值\n",
        "                   is_correct = (pred_num is not None and exp_num is not None and\n",
        "                               pred_num == exp_num)\n",
        "\n",
        "           # 有一个对就算对\n",
        "           if is_correct:\n",
        "               correct += 1\n",
        "\n",
        "           # Print evaluation details\n",
        "        #    print(\"\\nPrompt:\")\n",
        "        #    print(full_prompt)\n",
        "        #    print(\"\\nExpected Answer:\")\n",
        "        #    print(expected)\n",
        "        #    print(\"\\nExtracted Answer:\")\n",
        "        #    print(predicted)\n",
        "        #    print(\"\\nFull Generated Response:\")\n",
        "        #    print(response)\n",
        "        #    print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
        "        #    print(\"-\"*50)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(\"\\nFailed to parse model output for prompt:\")\n",
        "           print(full_prompt)\n",
        "           print(\"Error:\", e)\n",
        "           print(\"-\"*50)\n",
        "\n",
        "   # Calculate and print final accuracy\n",
        "   accuracy = (correct / total) * 100\n",
        "   print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
        "   print(\"=\"*50)\n",
        "\n",
        "   # Return model to training mode\n",
        "   model.train()\n",
        "   return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDQoSNkoOBtD",
      "metadata": {
        "id": "yDQoSNkoOBtD"
      },
      "source": [
        "在上述代码中：\n",
        "\n",
        "- `_extract_last_number` 从文本字符串中提取最后一个数值，确保它被正确分隔且没有多余的符号。\n",
        "- `_extract_single_number` 尝试从字符串中提取一个单一的数值，并且仅在找到一个数字时返回它。\n",
        "- `evaluate_model`：  \n",
        "  - 将模型设置为评估模式。\n",
        "  - 遍历每个评估示例，构建提示，对其进行分词，并生成响应。\n",
        "  - 提取预测的答案，并使用精确匹配和数值等价性检查（使用辅助函数）将其与预期答案进行比较。\n",
        "  - 记录并打印每个示例的详细评估信息，并计算总体准确率。\n",
        "\n",
        "## 第五部分：奖励函数\n",
        "\n",
        "在强化学习中，奖励函数通过提供对模型输出的反馈来指导训练过程。在我们的pipeline中，我们定义了两个奖励函数：\n",
        "\n",
        "1. **`correctness_reward`：**  \n",
        "   该函数根据生成的答案是否正确来分配奖励。它将从模型输出中提取的答案与预期答案进行比较，使用精确字符串匹配和数值等价性检查。精确匹配会获得更高的奖励（2.0），而基于数值等价性的匹配会获得较小的奖励（1.5）。\n",
        "   \n",
        "2. **`format_reward`：**  \n",
        "   该函数鼓励模型遵循所需的类似XML的输出格式。它为生成的文本中存在 `<reasoning>`、`</reasoning>`、`<answer>` 和 `</answer>` 标签提供少量的奖励。我们为这四个部分使用相对较小的值0.05，因为模型已经能够从前面的监督微调步骤中使用这些标签，所以我们给予这个小奖励，以确保它不会因为RL更新而忘记这样做。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "vuSh2MrHOBtD",
      "metadata": {
        "id": "vuSh2MrHOBtD"
      },
      "outputs": [],
      "source": [
        "# 准确性奖励(精准匹配2分, 答案匹配1.5分, 其余0分)\n",
        "def correctness_reward(prompts, completions, answer, **kwargs):\n",
        "    # 从模型输出中提取<answer>和</answer>之间的答案\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted = [extract_answer_from_model_output(r) for r in responses]\n",
        "    rewards = []\n",
        "    for r, a in zip(extracted, answer):\n",
        "        # 完全匹配给2分\n",
        "        if r == a:  # Exact match case\n",
        "            rewards.append(2.0)\n",
        "        # 不完全匹配给1.5分（数值答案正确）\n",
        "        else:    \n",
        "            # Try numeric equivalence\n",
        "            r_num = extract_single_number(str(r))\n",
        "            a_num = extract_single_number(str(a))\n",
        "            if r_num is not None and a_num is not None and r_num == a_num:\n",
        "                rewards.append(1.5)\n",
        "            else:\n",
        "                rewards.append(0.0)\n",
        "   # Log completion lengths\n",
        "    completion_lengths = [len(response.split()) for response in responses]\n",
        "    return rewards\n",
        "\n",
        "# 格式奖励(关于<reasoning> <reasoning> <answer> <answer>)\n",
        "def format_reward(completions, **kwargs):\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    rewards = []\n",
        "    format_scores = []\n",
        "    for response in responses:\n",
        "        score = 0.0\n",
        "        if \"<reasoning>\" in response: score += 0.2\n",
        "        if \"</reasoning>\" in response: score += 0.2\n",
        "        if \"<answer>\" in response: score += 0.2\n",
        "        if \"#### \" in response: score += 0.2\n",
        "        if \"</answer>\" in response: \n",
        "            score += 0.2\n",
        "            # 冗余输出惩罚\n",
        "            answer_end = response.find(\"</answer>\") + len(\"</answer>\")\n",
        "            extra_text = response[answer_end:].strip()\n",
        "            score = score * 0.5 if extra_text else score   # 如果\"</answer>\"后面还有别的输出，则格式分得分折半\n",
        "        rewards.append(score)\n",
        "        format_scores.append(score)\n",
        "    return rewards\n",
        "\n",
        "# 计算总得分\n",
        "def combined_reward(prompts, completions, answer):\n",
        "   \"\"\"\n",
        "   Combines correctness and format rewards.\n",
        "\n",
        "   Args:\n",
        "       prompts (list[str]): List of prompt texts\n",
        "       completions (list[list[dict]]): List of completion dictionaries\n",
        "       answer (list[str]): List of expected answers\n",
        "\n",
        "   Returns:\n",
        "       list[float]: Combined rewards for each prompt-completion pair\n",
        "\n",
        "   Explanation:\n",
        "       1. Calculates separate rewards for correctness and format compliance.\n",
        "       2. Combines the rewards with the following weights:\n",
        "          - Correctness score range: 0.0 to 2.0\n",
        "          - Format score range: 0.0 to 1.0\n",
        "          - Total possible range: 0.0 to 3.0\n",
        "       3. Returns the combined reward for each example.\n",
        "   \"\"\"\n",
        "   # Get individual rewards\n",
        "   correctness_scores = correctness_reward(prompts=prompts, completions=completions, answer=answer)\n",
        "   format_scores = format_reward(completions=completions)\n",
        "\n",
        "   # Combine rewards - correctness is weighted more heavily\n",
        "   combined_rewards = []\n",
        "   for c_score, f_score in zip(correctness_scores, format_scores):\n",
        "       # Correctness score range: 0.0 to 2.0\n",
        "       # Format score range: 0.0 to 1.0\n",
        "       # Total range: 0.0 to 2.8\n",
        "       combined_rewards.append(c_score + f_score)\n",
        "\n",
        "   return combined_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nyAyi1KOmlyw",
      "metadata": {
        "id": "nyAyi1KOmlyw"
      },
      "source": [
        "## 第六部分：从零开始实现DataParallel GRPO\n",
        "\n",
        "在本节中，我们从零开始实现GRPO算法的所有构建模块。该实现假设运行代码的机器至少具有2个GPU。我们使用PyTorch的 `DataParallel` API 将策略模型分布在GPU核心上，每个GPU核心上运行一个模型副本。批次数据在GPU核心之间进行分割。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "506f9b3c-539e-4abb-9e57-4ee4ec669cbc",
      "metadata": {
        "id": "506f9b3c-539e-4abb-9e57-4ee4ec669cbc"
      },
      "outputs": [],
      "source": [
        "def selective_log_softmax(logits, input_ids):\n",
        "    \"\"\"\n",
        "    计算词汇表中特定 token 的对数概率。\n",
        "\n",
        "    参数:\n",
        "        logits (torch.Tensor): 模型输出的原始 logits（未归一化）。\n",
        "        input_ids (torch.Tensor): 需要获取对数概率的目标 token ID。\n",
        "\n",
        "    返回:\n",
        "        torch.Tensor: 选中 token 的对数概率，形状与 `input_ids` 一致。\n",
        "\n",
        "    实现步骤:\n",
        "        1. 对 logits 应用 log_softmax，得到词汇表维度上的对数概率分布。\n",
        "        2. 使用 `gather` 提取与 input_ids 对应的特定 token 的对数概率。\n",
        "        3. 压缩多余的维度，使输出形状与 input_ids 匹配。\n",
        "    \"\"\"\n",
        "    # 对 logits 应用 log_softmax（沿词汇维度归一化）\n",
        "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
        "    \n",
        "    # 从 log_probs 中提取 input_ids 对应的值，并压缩最后一个维度\n",
        "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "def compute_log_probs(model, input_ids, attention_mask, logits_to_keep):\n",
        "    \"\"\"\n",
        "    计算输入序列末尾指定数量 token 的对数概率。\n",
        "\n",
        "    参数:\n",
        "        model: 语言模型（如 HuggingFace 的预训练模型）。\n",
        "        input_ids (torch.Tensor): 输入序列的 token ID，形状为 [batch_size, seq_len]。\n",
        "        attention_mask (torch.Tensor): 注意力掩码，标识有效 token 位置，形状与 input_ids 一致。\n",
        "        logits_to_keep (int): 需要计算对数概率的末尾 token 数量（保留最后几个 token）。\n",
        "\n",
        "    返回:\n",
        "        torch.Tensor: 选中 token 的对数概率，形状为 [batch_size, logits_to_keep]。\n",
        "\n",
        "    实现步骤:\n",
        "        1. 获取模型输出的 logits，并移除最后一个位置的预测（因预测的是下一个 token）。\n",
        "        2. 从 input_ids 中截取最后 `logits_to_keep` 个 token 作为目标。\n",
        "        3. 从 logits 中截取对应的最后 `logits_to_keep` 个位置的预测结果。\n",
        "        4. 调用 `selective_log_softmax` 计算目标 token 的对数概率。\n",
        "    \"\"\"\n",
        "    # 获取模型输出 logits，并移除最后一个 token 的预测（形状变为 [batch_size, seq_len-1, vocab_size]）\n",
        "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :]\n",
        "    \n",
        "    # 截取 input_ids 的最后 logits_to_keep 个 token（目标 token）\n",
        "    input_ids = input_ids[:, -logits_to_keep:]\n",
        "    \n",
        "    # 截取 logits 的最后 logits_to_keep 个位置（对应目标 token 的预测）\n",
        "    logits = logits[:, -logits_to_keep:, :]\n",
        "    \n",
        "    # 计算目标 token 的对数概率\n",
        "    return selective_log_softmax(logits, input_ids)\n",
        "\n",
        "\n",
        "\n",
        "def create_completion_mask(completion_ids, eos_token_id):\n",
        "    \"\"\"\n",
        "    生成一个掩码，用于屏蔽生成序列中首个 EOS token 之后的所有 token。\n",
        "\n",
        "    参数:\n",
        "        completion_ids (torch.Tensor): 生成序列的 token ID，形状为 [batch_size, seq_len]。\n",
        "        eos_token_id (int): 结束符（EOS）的 token ID。\n",
        "\n",
        "    返回:\n",
        "        torch.Tensor: 二进制掩码，形状为 [batch_size, seq_len]，有效 token 为 1，EOS 之后为 0。\n",
        "\n",
        "    实现步骤:\n",
        "        1. 标记 EOS 出现的位置。\n",
        "        2. 初始化默认 EOS 索引为序列长度（处理无 EOS 的情况）。\n",
        "        3. 确定哪些序列中存在 EOS。\n",
        "        4. 对有 EOS 的序列，更新首个 EOS 的位置索引。\n",
        "        5. 生成位置索引矩阵，并与 EOS 索引比较生成掩码。\n",
        "    \"\"\"\n",
        "    # 标记 EOS token 的位置（布尔张量）\n",
        "    is_eos = completion_ids == eos_token_id\n",
        "\n",
        "    # 初始化默认 EOS 索引为序列长度（若无 EOS，则保留全部 token）\n",
        "    eos_idx = torch.full(\n",
        "        (is_eos.size(0),), \n",
        "        is_eos.size(1),  # 默认值为序列长度 seq_len\n",
        "        dtype=torch.long, \n",
        "        device=completion_ids.device\n",
        "    )\n",
        "\n",
        "    # 检查每个序列是否存在至少一个 EOS\n",
        "    mask_exists = is_eos.any(dim=1)\n",
        "\n",
        "    # 对有 EOS 的序列，获取首个 EOS 的位置索引\n",
        "    # （利用 argmax 返回第一个 True 的位置）\n",
        "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
        "\n",
        "    # 生成位置索引矩阵 [batch_size, seq_len]\n",
        "    # 例如：若 seq_len=3，则每行为 [0, 1, 2]\n",
        "    sequence_indices = torch.arange(\n",
        "        is_eos.size(1), \n",
        "        device=completion_ids.device\n",
        "    ).expand(is_eos.size(0), -1)\n",
        "\n",
        "    # 生成掩码：位置索引 <= 首个 EOS 索引的位置设为 1\n",
        "    return (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
        "\n",
        "\n",
        "\n",
        "def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32):\n",
        "    \"\"\"\n",
        "    为每个提示(prompt)生成多个补全(completion)。\n",
        "\n",
        "    Args:\n",
        "        model: 语言模型对象\n",
        "        tokenizer: 用于文本编码/解码的分词器\n",
        "        prompts (list): 文本提示列表\n",
        "        num_generations (int): 每个提示生成的补全数量\n",
        "        max_completion_length (int): 生成的最大token数量\n",
        "\n",
        "    Returns:\n",
        "        tuple: 包含四个元素的元组，格式为 (prompt_ids, prompt_mask, completion_ids, completion_mask)\n",
        "\n",
        "    Explanation:\n",
        "        1. 编码输入提示(prompts)并将其移动到计算设备\n",
        "        2. 每个提示重复多次以实现批量生成\n",
        "        3. 使用模型生成补全内容\n",
        "        4. 从输出中分离生成的补全部分\n",
        "        5. 创建补全的注意力掩码\n",
        "    \"\"\"\n",
        "    # 选择计算设备（优先使用GPU）\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # 编码输入提示(prompts)\n",
        "    inputs = tokenizer(\n",
        "        prompts, \n",
        "        return_tensors=\"pt\",  # 返回PyTorch tensor格式\n",
        "        padding=True,         # 启用填充\n",
        "        padding_side=\"left\"   # 左侧填充\n",
        "    )\n",
        "    \n",
        "    # 将输入数据移动到计算设备\n",
        "    prompt_ids = inputs[\"input_ids\"].to(device)\n",
        "    prompt_mask = inputs[\"attention_mask\"].to(device)\n",
        "    \n",
        "    # 打印调试信息（输入批量大小和设备）\n",
        "    # print(f\"Input batch size: {prompt_ids.size(0)}, Device before model: {prompt_ids.device}\")\n",
        "    \n",
        "    # 记录提示文本的原始长度（用于后续截取生成的补全）\n",
        "    prompt_length = prompt_ids.size(1)\n",
        "    \n",
        "    # 沿批次维度(batch dimension)重复每个提示\n",
        "    # 示例：num_generations=4时将批次扩大4倍（每个原始样本生成4个补全）\n",
        "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
        "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0)\n",
        "    \n",
        "    # 使用模型生成文本\n",
        "    outputs = model.generate(\n",
        "        input_ids=prompt_ids,\n",
        "        attention_mask=prompt_mask,\n",
        "        max_new_tokens=max_completion_length,  # 生成的最大新token数\n",
        "        do_sample=True,         # 启用随机采样\n",
        "        temperature=1.0,        # 采样温度参数\n",
        "        pad_token_id=tokenizer.pad_token_id,   # 填充token ID\n",
        "        eos_token_id=tokenizer.eos_token_id,   # 结束token ID\n",
        "        early_stopping=False    # 禁用提前停止（生成指定长度）\n",
        "    )\n",
        "    \n",
        "    # 打印生成后的调试信息\n",
        "    print(f\"Output batch size: {outputs.size(0)}, Device after model: {outputs.device}\")\n",
        "    \n",
        "    # 提取生成的补全部分（截取掉原始的提示内容）\n",
        "    completion_ids = outputs[:, prompt_length:]\n",
        "    \n",
        "    # 创建补全的注意力掩码（用于后续处理）\n",
        "    completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id)\n",
        "    \n",
        "    return prompt_ids, prompt_mask, completion_ids, completion_mask\n",
        "\n",
        "\n",
        "\n",
        "def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length):\n",
        "    \"\"\"\n",
        "    生成用于GRPO训练的交互数据，包括生成文本及其对数概率。\n",
        "\n",
        "    Args:\n",
        "        model: 当前训练的强化学习策略模型\n",
        "        ref_model: 参考模型（用于KL散度计算的基准模型）\n",
        "        tokenizer: 文本编码/解码器\n",
        "        batch_samples (list): 训练样本组成的批次（每个样本包含prompt和answer）\n",
        "        num_generations (int): 每个样本生成的补全数量\n",
        "        max_completion_length (int): 生成文本的最大token长度\n",
        "\n",
        "    Returns:\n",
        "        dict: 包含GRPO训练所需完整数据的字典\n",
        "\n",
        "    Explanation:\n",
        "        1. 解析输入批次中的提示和标准答案\n",
        "        2. 使用当前模型生成多条补全文本\n",
        "        3. 拼接提示与生成文本得到完整序列\n",
        "        4. 计算策略模型和参考模型的生成文本对数概率\n",
        "        5. 格式化生成文本以计算奖励\n",
        "        6. 对齐输入数据维度（根据生成数量扩展样本）\n",
        "    \"\"\"\n",
        "    # 检测计算设备（优先使用GPU）\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # 从训练样本中提取提示和答案\n",
        "    # 支持字典和元组两种数据格式\n",
        "    # 示例：\n",
        "    # 'prompt': 'Respond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?'\n",
        "    # 'answer': '72'\n",
        "    prompts = [sample[\"prompt\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples]\n",
        "    answers = [sample[\"answer\"] if isinstance(sample, dict) else sample[1] for sample in batch_samples]\n",
        "\n",
        "    # 禁用梯度计算（生成阶段）\n",
        "    with torch.no_grad():\n",
        "        # 生成多个补全文本 (shape: (batch_size * num_generations, ...))\"\n",
        "        prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(\n",
        "            model, tokenizer, prompts, num_generations, max_completion_length\n",
        "        )\n",
        "        \n",
        "        # 拼接提示和补全作为完整输入序列\n",
        "        # 维度: (batch_size*num_generations, prompt_len+completion_len)\n",
        "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)             \n",
        "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)     # 相同的维度结构\n",
        "        \n",
        "        # 记录需要保留的logits数量（补全文本的token长度）\n",
        "        logits_to_keep = completion_ids.size(1)\n",
        "        \n",
        "        # 计算模型对生成内容的对数概率\n",
        "        old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)       # 当前策略模型\n",
        "        ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep)   # 参考模型\n",
        "\n",
        "    # 解码生成文本用于后续奖励计算\n",
        "    formatted_completions = [[\n",
        "        {'content': tokenizer.decode(ids, skip_special_tokens=True)}  # 解码并跳过特殊token\n",
        "    ] for ids in completion_ids]  # 每个生成样本包装成字典列表形式\n",
        "\n",
        "    # 扩展原始样本以匹配生成的补全数量\n",
        "    repeated_prompts = [p for p in prompts for _ in range(num_generations)]    # 格式示例：[prompt1, prompt1, prompt1..., prompt2, prompt2...]\n",
        "    repeated_answers = [a for a in answers for _ in range(num_generations)]   # 同理扩展答案\n",
        "\n",
        "    # 封装所有训练数据\n",
        "    return {\n",
        "        \"input_ids\": input_ids,                    # 完整输入序列的token ID\n",
        "        \"attention_mask\": attention_mask,          # 对应的注意力掩码\n",
        "        \"completion_mask\": completion_mask,        # 仅补全部分的掩码\n",
        "        \"old_log_probs\": old_log_probs,            # 策略模型的对数概率\n",
        "        \"ref_log_probs\": ref_log_probs,            # 参考模型的对数概率（用于KL约束）\n",
        "        \"formatted_completions\": formatted_completions,  # 可用于奖励模型的可读文本\n",
        "        \"repeated_prompts\": repeated_prompts,      # 扩展后的提示文本（与生成样本对齐）\n",
        "        \"repeated_answers\": repeated_answers,      # 扩展后的答案（与生成样本对齐）\n",
        "        \"logits_to_keep\": logits_to_keep,          # 补全文本的token长度\n",
        "        \"batch_size\": len(prompts),                # 原始批次数目\n",
        "        \"num_generations\": num_generations         # 每个样本生成数\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2):\n",
        "    \"\"\"\n",
        "    计算GRPO（Generalized Reinforcement Policy Optimization）损失函数。\n",
        "\n",
        "    Args:\n",
        "        model: 当前训练的策略模型\n",
        "        ref_model: 参考模型（用于KL散度计算）\n",
        "        rollout_data (dict): 由generate_rollout_data生成的交互数据\n",
        "        tokenizer: 文本编码/解码器\n",
        "        reward_function: 奖励计算函数\n",
        "        beta (float): KL散度惩罚系数（默认0.01）\n",
        "        epsilon (float): PPO裁剪范围参数（默认0.2）\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 需要最小化的GRPO损失值\n",
        "        float: 当前批次的平均奖励值\n",
        "\n",
        "    Explanation:\n",
        "        实现GRPO算法的核心损失计算，包含：\n",
        "        - 策略梯度优化（PPO裁剪机制）\n",
        "        - KL散度约束（防止策略过激偏离参考模型）\n",
        "        - 奖励标准化处理\n",
        "    \"\"\"\n",
        "    # 自动检测计算设备\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # 从交互数据中提取关键元素\n",
        "    input_ids = rollout_data[\"input_ids\"]  # 完整输入序列的token ID (batch_size*num_gen, seq_len)\n",
        "    attention_mask = rollout_data[\"attention_mask\"]  # 注意力掩码\n",
        "    completion_mask = rollout_data[\"completion_mask\"]  # 补全部分的掩码（仅生成部分为1）\n",
        "    logits_to_keep = rollout_data[\"logits_to_keep\"]  # 补全文本的token长度\n",
        "    old_log_probs = rollout_data[\"old_log_probs\"]  # 旧策略的对数概率\n",
        "    ref_log_probs = rollout_data[\"ref_log_probs\"]  # 参考模型的对数概率\n",
        "\n",
        "    # 1. 计算当前策略模型的对数概率, 梯度可追踪\n",
        "    token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
        "    \n",
        "    # 2. 新旧策略概率比率（指数运算将log概率转换为概率比值）\n",
        "    ratio = torch.exp(token_log_probs - old_log_probs)  # 形状: (batch_size*num_gen, seq_len)\n",
        "\n",
        "    # 3. 计算奖励值 \n",
        "    rewards = torch.tensor(\n",
        "        reward_function(\n",
        "            prompts=rollout_data[\"repeated_prompts\"],  # 扩展后的提示列表\n",
        "            completions=rollout_data[\"formatted_completions\"],  # 格式化的生成文本\n",
        "            answer=rollout_data[\"repeated_answers\"]  # 扩展后的标准答案\n",
        "        ),    # 把模型的输出输入到奖励计算函数，即combined_reward\n",
        "        dtype=torch.float32,\n",
        "        device=device\n",
        "    )  \n",
        "\n",
        "    # 4. 维度调整与奖励标准化 （广义优势函数）\n",
        "    batch_size = rollout_data[\"batch_size\"]  # 原始批次数\n",
        "    num_generations = rollout_data[\"num_generations\"]  # 每个样本生成数\n",
        "    \n",
        "    # 将奖励重塑为矩阵形式： (batch_size, num_generations)\n",
        "    rewards = rewards.view(batch_size, num_generations)\n",
        "    \n",
        "    # 计算并打印平均奖励（用于监控训练进度）\n",
        "    avg_reward = rewards.mean().item()\n",
        "    print(\"Average Reward:\", avg_reward)\n",
        "    \n",
        "    # 计算每个提示对应的平均奖励和标准差（用于标准化）\n",
        "    mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations)  # (batch_size*num_gen,)\n",
        "    std_rewards = rewards.std(dim=1).repeat_interleave(num_generations)    \n",
        "    \n",
        "    # 优势函数计算（标准化处理）\n",
        "    advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1)  \n",
        "\n",
        "    # 5. PPO裁剪目标计算 \n",
        "    surr1 = ratio * advantages          # 未裁剪的原始目标\n",
        "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages  # 裁剪后的保守目标\n",
        "    surrogate_loss = torch.min(surr1, surr2)  # 取两者较小值 → 形状: (batch_size*num_gen, seq_len)\n",
        "\n",
        "    # 6. KL散度惩罚项计算 \n",
        "    # 使用近似公式：KL(p||q) ≈ exp(log_p - log_q) - (log_p - log_q) - 1\n",
        "    kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1\n",
        "\n",
        "    # 7. 综合损失计算 \n",
        "    per_token_loss = surrogate_loss - beta * kl  \n",
        "    \n",
        "    # 应用补全掩码（仅计算生成部分的损失）并平均\n",
        "    masked_loss = per_token_loss * completion_mask  # 形状保持不变\n",
        "    loss = -((masked_loss.sum(dim=1) / completion_mask.sum(dim=1)).mean())  # 负号因为要最大化奖励\n",
        "    \n",
        "    return loss, avg_reward\n",
        "\n",
        "# 1. 导入PEFT库进行LoRA配置\n",
        "from peft import LoraConfig, get_peft_model\n",
        "def train_with_grpo(model, tokenizer, train_data, num_iterations=1, num_steps=500, batch_size=4,\n",
        "                              num_generations=4, max_completion_length=128, beta=0.1,\n",
        "                              learning_rate=5e-6, mu=3, epsilon=0.2, reward_function=None, device_ids=None):\n",
        "    \"\"\"\n",
        "    使用GRPO算法进行迭代式强化学习训练。\n",
        "\n",
        "    Args:\n",
        "        model: 待训练的语言模型\n",
        "        tokenizer: 文本编码/解码器\n",
        "        train_data (list): 训练数据集\n",
        "        num_iterations (int): 外层迭代次数（参考模型更新次数）\n",
        "        num_steps (int): 每个迭代的批次更新次数\n",
        "        batch_size (int): 每批次的提示样本数\n",
        "        num_generations (int): 每个提示生成的补全数量\n",
        "        max_completion_length (int): 生成文本的最大token长度\n",
        "        beta (float): KL散度惩罚系数\n",
        "        learning_rate (float): 优化器学习率\n",
        "        mu (int): 每个批次的策略更新次数\n",
        "        epsilon (float): PPO裁剪参数\n",
        "        reward_function: 奖励计算函数\n",
        "        device_ids (list): 多GPU设备ID列表\n",
        "\n",
        "    Returns:\n",
        "        训练完成的模型\n",
        "\n",
        "    Explanation:\n",
        "        实现GRPO算法的完整训练流程，包含：\n",
        "        - 多GPU并行训练支持\n",
        "        - 参考模型的定期更新\n",
        "        - 多轮策略优化步骤\n",
        "        - 训练指标监控\n",
        "    \"\"\"\n",
        "    # 断言确保多GPU配置有效\n",
        "    assert device_ids is not None and len(device_ids) > 1, \"需要至少2个GPU核心运行本代码!\"  \n",
        "\n",
        "    # 配置tensorboard \n",
        "    writer = SummaryWriter(log_dir=\"runs/dlc_1.5B_experiment\")  \n",
        "\n",
        "    # 自动检测计算设备\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 多GPU并行处理设置 \n",
        "    # 2. 配置LoRA参数\n",
        "    # lora_config = LoraConfig(\n",
        "    #     r=8,  # 秩\n",
        "    #     lora_alpha=32,\n",
        "    #     target_modules=[\"q_proj\", \"v_proj\"],  # 根据模型结构调整\n",
        "    #     lora_dropout=0.1,\n",
        "    #     bias=\"none\",\n",
        "    #     task_type=\"CAUSAL_LM\"\n",
        "    # )\n",
        "    lora_config = LoraConfig(\n",
        "                                r=8,\n",
        "                                lora_alpha=16,\n",
        "                                lora_dropout=0.2,\n",
        "                                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # 增加k_proj\n",
        "                                bias=\"all\",\n",
        "                                task_type=\"CAUSAL_LM\",\n",
        "                                modules_to_save=[\"lm_head\"]  # 解锁输出层\n",
        "                            )\n",
        "\n",
        "    # 3. 获取新训练模型\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters() \n",
        "    model = nn.DataParallel(model, device_ids=device_ids)  # 包装为数据并行模型\n",
        "    print(f\"模型已部署到多GPU: {device_ids}\")\n",
        "    \n",
        "    # 外层循环：参考模型更新迭代 \n",
        "    for iteration in range(num_iterations):\n",
        "        print(f\"\\n当前迭代 {iteration+1}/{num_iterations}\")\n",
        "\n",
        "        # 创建参考模型（深拷贝当前模型）\n",
        "        ref_model = copy.deepcopy(model.module)  # 注意通过.module访问原始模型\n",
        "        ref_model.eval()  # 设置为评估模式\n",
        "        for param in ref_model.parameters():\n",
        "            param.requires_grad = False  # 冻结参考模型参数\n",
        "        print(\"参考模型创建完成.\")\n",
        "\n",
        "        # 优化器重新初始化 -----------------------------------------------------\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "        model.train()  # 确保策略模型处于训练模式\n",
        "\n",
        "        # 内层循环：训练批次处理 ------------------------------------------------\n",
        "        for step in range(num_steps):\n",
        "            # 1. 采样训练批次\n",
        "            batch_samples = random.sample(train_data, batch_size)\n",
        "            \n",
        "            # 2. 生成交互数据（禁用梯度计算）\n",
        "            with torch.no_grad():\n",
        "                rollout_data = generate_rollout_data(\n",
        "                    model.module,  # 使用原始模型（非DataParallel包装）\n",
        "                    ref_model,\n",
        "                    tokenizer,\n",
        "                    batch_samples,\n",
        "                    num_generations,\n",
        "                    max_completion_length\n",
        "                )\n",
        "            \n",
        "            # 3. 多轮策略优化（mu次参数更新）\n",
        "            for grpo_iter in range(mu):\n",
        "                # 计算GRPO损失\n",
        "                loss, avg_reward = grpo_loss(\n",
        "                    model.module,\n",
        "                    ref_model,\n",
        "                    rollout_data,\n",
        "                    tokenizer,\n",
        "                    reward_function,\n",
        "                    beta=beta,\n",
        "                    epsilon=epsilon\n",
        "                )\n",
        "                \n",
        "                # 梯度更新步骤\n",
        "                optimizer.zero_grad()  # 清空梯度\n",
        "                loss.backward()  # 反向传播\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)  # 梯度裁剪\n",
        "                optimizer.step()  # 参数更新\n",
        "                \n",
        "                # wandb.log({\n",
        "                #     \"loss\": loss.item(),\n",
        "                #     \"average_reward\": avg_reward,\n",
        "                #     \"iteration\": iteration + 1,\n",
        "                #     \"step\": step + 1,\n",
        "                #     \"grpo_iter\": grpo_iter + 1\n",
        "                # })\n",
        "                # 替换 wandb.log 的代码为：\n",
        "                global_step = iteration * num_steps + step  # 需要计算全局步数\n",
        "                writer.add_scalar('Loss/train', loss.item(), global_step) \n",
        "                writer.add_scalar('Reward/train', avg_reward, global_step)\n",
        "\n",
        "                \n",
        "                # 打印训练进度\n",
        "                print(f\"迭代 [{iteration+1}/{num_iterations}], 步骤 [{step+1}/{num_steps}], \"\n",
        "                      f\"GRPO更新 [{grpo_iter+1}/{mu}], 损失: {loss.item():.4f}\")\n",
        "                print()\n",
        "                \n",
        "                # GPU监控（可选）\n",
        "                # for i in range(torch.cuda.device_count()):\n",
        "                #    print(f\"GPU {i} 内存使用: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MiB, \"\n",
        "                #          f\"利用率: {torch.cuda.utilization(i)}%\")\n",
        "    \n",
        "    # 训练完成关闭 writer\n",
        "    writer.close()\n",
        "\n",
        "    # 返回原始模型（解除DataParallel包装）\n",
        "    return model.module\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a28e94-aa45-4ac1-a17a-bae82a415cbd",
      "metadata": {
        "id": "05a28e94-aa45-4ac1-a17a-bae82a415cbd"
      },
      "source": [
        "## 第七部分：训练设置与执行\n",
        "\n",
        "在本节中，我们将所有组件整合在一起以设置并运行训练。我们首先加载预训练模型和分词器，准备评估数据，然后使用我们上面从零开始实现的 `train_with_grpo` 进行强化学习（RL）微调。\n",
        "\n",
        "关键步骤包括：\n",
        "\n",
        "- **模型和分词器初始化：**  \n",
        "  加载模型 `\"Qwen/Qwen2.5-1.5B-Instruct\"` 并应用优化设置（使用 `torch.bfloat16` 和 FlashAttention2）。同时加载分词器，并将其填充标记设置为序列结束标记。使用 `torch.bfloat16` 加载模型会将其参数从每个数字32位转换为16位，从而将模型的内存使用量减半，并可以在现代GPU上加快训练速度。\n",
        "  \n",
        "- **初始评估：**  \n",
        "  在微调之前，对模型进行一些示例的评估，以建立基线性能。\n",
        "    \n",
        "- **强化学习微调（RL）：**  \n",
        "  使用我们实现的 `train_with_grpo` 函数进行GRPO训练，配置适当的训练参数和奖励函数。然后在剩余的训练数据上进行RL训练。\n",
        "  \n",
        "- **最终评估和模型保存：**  \n",
        "  RL微调完成后，再次评估模型，并保存最终的模型。\n",
        "\n",
        "在以下代码中：\n",
        "  \n",
        "- 确定设备（优先使用GPU，否则使用CPU）。\n",
        "- 加载预训练的Qwen2.5-1.5B-Instruct模型和分词器。分词器的填充标记设置为eos_token。\n",
        "- 保留数据集的一小部分用于评估，以提供基线。\n",
        "- 通过启用梯度检查点和禁用KV缓存来优化模型的内存效率。\n",
        "- **步骤1：** 在微调之前评估模型，以建立基线准确率。\n",
        "- **步骤2：** 使用 `train_with_grpo` 函数进行强化学习微调，结合我们定义的奖励函数（`format_reward` 和 `correctness_reward`，组合为 `combined_reward`）。模型使用多GPU进行训练。\n",
        "- **步骤3：** 将最终微调后的模型和分词器保存到磁盘。\n",
        "\n",
        "我们为GRPO训练管道使用了以下超参数：\n",
        "\n",
        "### **训练配置**\n",
        "\n",
        "这些参数配置了使用GRPO算法进行强化学习微调的运行。我们设置如下：\n",
        "\n",
        "- **num_iterations=1**  \n",
        "  外部迭代次数，每次迭代从当前策略模型创建一个新的参考模型。一次迭代是对整个数据集的一次遍历。\n",
        "\n",
        "- **num_steps=500**  \n",
        "  训练循环最多执行500步，每步处理一批示例。\n",
        "\n",
        "- **batch_size=7**  \n",
        "  每步处理7个示例，在8个GPU的情况下，每个GPU处理1个示例。`DataParallel` 使用一个GPU（0）作为主GPU，用于聚合梯度和收集输出。\n",
        "\n",
        "- **num_generations=14**  \n",
        "  对于训练数据中的每个提示，训练器将生成14个不同的完成结果。这些多个生成结果用于计算相对优势（或奖励信号），以指导RL更新。如果GPU的VRAM较小，可以减少此数值。\n",
        "\n",
        "- **max_completion_length=400**  \n",
        "  生成完成结果（序列的“响应”部分）时，生成的长度限制为400个标记。这限制了RL阶段模型生成的输出长度。如果GPU的VRAM较小，可以减少此数值。\n",
        "\n",
        "- **beta=0.04**  \n",
        "  GRPO损失函数中KL散度惩罚的系数。这控制了模型与参考模型的偏离程度。\n",
        "\n",
        "- **learning_rate=5e-6**  \n",
        "  RL微调的学习率。使用相对较低的学习率以确保策略更新的稳定性。\n",
        "\n",
        "- **mu=1**  \n",
        "  每批滚动数据执行的策略更新次数。在我们的情况下，每批只执行一次更新。\n",
        "\n",
        "- **epsilon=0.1**  \n",
        "  GRPO中PPO组件的裁剪参数。这防止策略在单次更新中发生过大变化。\n",
        "\n",
        "在微调前后对模型进行评估，以衡量准确率的提升。最后，将微调后的模型保存到 \"grpo_finetuned_model\" 目录中。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2f833c95",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the latest cached version of the dataset since openai/gsm8k couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
            "Found the latest cached dataset configuration 'main' at dataset/openai___gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Wed Mar  5 02:24:00 2025).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------打印示例数据----------\n",
            "question: \n",
            " Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "answer: \n",
            " Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72\n",
            "\n",
            "Number of training examples: 7443\n"
          ]
        }
      ],
      "source": [
        "all_data = prepare_dataset(\"train\")\n",
        "random.shuffle(all_data)\n",
        "size_of_eval_data = 30 # change to a smaller value to save time or to a larger number for a more reliable estimate\n",
        "eval_data = all_data[:size_of_eval_data]\n",
        "train_data = all_data[size_of_eval_data:]\n",
        "print()\n",
        "print(f\"Number of training examples: {len(train_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Oj2_ZDHCOBtD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Oj2_ZDHCOBtD",
        "outputId": "a5ac192f-b874-4718-cce9-23e73f09cc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using primary device: cuda:0\n",
            "Downloading model...\n",
            "Model downloaded\n",
            "Detected 4 GPUs\n",
            "\n",
            "Starting RL fine-tuning using GRPO...\n",
            "Train Config:\n",
            "{'num_iterations': 1, 'num_steps': 500, 'batch_size': 1, 'num_generations': 14, 'max_completion_length': 400, 'beta': 0.04, 'learning_rate': 5e-06, 'mu': 1, 'epsilon': 0.1}\n",
            "Weights & Biases initialized.\n",
            "trainable params: 234,921,984 || all params: 1,778,578,944 || trainable%: 13.2084\n",
            "模型已部署到多GPU: [0, 1, 2, 3]\n",
            "\n",
            "当前迭代 1/1\n",
            "参考模型创建完成.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22142858803272247\n",
            "迭代 [1/1], 步骤 [1/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2071428894996643\n",
            "迭代 [1/1], 步骤 [2/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6357143521308899\n",
            "迭代 [1/1], 步骤 [3/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.29285717010498047\n",
            "迭代 [1/1], 步骤 [4/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22857144474983215\n",
            "迭代 [1/1], 步骤 [5/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3857143521308899\n",
            "迭代 [1/1], 步骤 [6/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [7/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2857142984867096\n",
            "迭代 [1/1], 步骤 [8/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.15000002086162567\n",
            "迭代 [1/1], 步骤 [9/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5071429014205933\n",
            "迭代 [1/1], 步骤 [10/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.08571429550647736\n",
            "迭代 [1/1], 步骤 [11/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.44285714626312256\n",
            "迭代 [1/1], 步骤 [12/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3857143223285675\n",
            "迭代 [1/1], 步骤 [13/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5428571701049805\n",
            "迭代 [1/1], 步骤 [14/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.18571430444717407\n",
            "迭代 [1/1], 步骤 [15/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.25\n",
            "迭代 [1/1], 步骤 [16/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.08571429550647736\n",
            "迭代 [1/1], 步骤 [17/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22142858803272247\n",
            "迭代 [1/1], 步骤 [18/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.44285717606544495\n",
            "迭代 [1/1], 步骤 [19/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.10000000894069672\n",
            "迭代 [1/1], 步骤 [20/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6714286208152771\n",
            "迭代 [1/1], 步骤 [21/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22857145965099335\n",
            "迭代 [1/1], 步骤 [22/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1214287281036377\n",
            "迭代 [1/1], 步骤 [23/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40000003576278687\n",
            "迭代 [1/1], 步骤 [24/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.16428573429584503\n",
            "迭代 [1/1], 步骤 [25/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [26/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3500000238418579\n",
            "迭代 [1/1], 步骤 [27/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.25\n",
            "迭代 [1/1], 步骤 [28/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.10000000894069672\n",
            "迭代 [1/1], 步骤 [29/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.357142835855484\n",
            "迭代 [1/1], 步骤 [30/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6071428656578064\n",
            "迭代 [1/1], 步骤 [31/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.16428573429584503\n",
            "迭代 [1/1], 步骤 [32/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.44285720586776733\n",
            "迭代 [1/1], 步骤 [33/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5571429133415222\n",
            "迭代 [1/1], 步骤 [34/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.36428576707839966\n",
            "迭代 [1/1], 步骤 [35/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2428571581840515\n",
            "迭代 [1/1], 步骤 [36/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05000000074505806\n",
            "迭代 [1/1], 步骤 [37/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.535714328289032\n",
            "迭代 [1/1], 步骤 [38/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.10000000894069672\n",
            "迭代 [1/1], 步骤 [39/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.27142858505249023\n",
            "迭代 [1/1], 步骤 [40/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.25714290142059326\n",
            "迭代 [1/1], 步骤 [41/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5500000715255737\n",
            "迭代 [1/1], 步骤 [42/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22857144474983215\n",
            "迭代 [1/1], 步骤 [43/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22142857313156128\n",
            "迭代 [1/1], 步骤 [44/500], GRPO更新 [1/1], 损失: -0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3285714387893677\n",
            "迭代 [1/1], 步骤 [45/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [46/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [47/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.23571431636810303\n",
            "迭代 [1/1], 步骤 [48/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.09285714477300644\n",
            "迭代 [1/1], 步骤 [49/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0\n",
            "迭代 [1/1], 步骤 [50/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6857143640518188\n",
            "迭代 [1/1], 步骤 [51/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.30000004172325134\n",
            "迭代 [1/1], 步骤 [52/500], GRPO更新 [1/1], 损失: -0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2071428745985031\n",
            "迭代 [1/1], 步骤 [53/500], GRPO更新 [1/1], 损失: -0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6000000238418579\n",
            "迭代 [1/1], 步骤 [54/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.07857143133878708\n",
            "迭代 [1/1], 步骤 [55/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1785714477300644\n",
            "迭代 [1/1], 步骤 [56/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2428571581840515\n",
            "迭代 [1/1], 步骤 [57/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [58/500], GRPO更新 [1/1], 损失: -0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2071428745985031\n",
            "迭代 [1/1], 步骤 [59/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.47142863273620605\n",
            "迭代 [1/1], 步骤 [60/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2071428745985031\n",
            "迭代 [1/1], 步骤 [61/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.18571430444717407\n",
            "迭代 [1/1], 步骤 [62/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4500000476837158\n",
            "迭代 [1/1], 步骤 [63/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [64/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.428571492433548\n",
            "迭代 [1/1], 步骤 [65/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5\n",
            "迭代 [1/1], 步骤 [66/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0357142873108387\n",
            "迭代 [1/1], 步骤 [67/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [68/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.33571428060531616\n",
            "迭代 [1/1], 步骤 [69/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.535714328289032\n",
            "迭代 [1/1], 步骤 [70/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.15000002086162567\n",
            "迭代 [1/1], 步骤 [71/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1785714477300644\n",
            "迭代 [1/1], 步骤 [72/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.18571428954601288\n",
            "迭代 [1/1], 步骤 [73/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7000000476837158\n",
            "迭代 [1/1], 步骤 [74/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3142857253551483\n",
            "迭代 [1/1], 步骤 [75/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5285714864730835\n",
            "迭代 [1/1], 步骤 [76/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3928571939468384\n",
            "迭代 [1/1], 步骤 [77/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [78/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0714285746216774\n",
            "迭代 [1/1], 步骤 [79/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.25\n",
            "迭代 [1/1], 步骤 [80/500], GRPO更新 [1/1], 损失: -0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.23571428656578064\n",
            "迭代 [1/1], 步骤 [81/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.10000000894069672\n",
            "迭代 [1/1], 步骤 [82/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [83/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.30000001192092896\n",
            "迭代 [1/1], 步骤 [84/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5214285850524902\n",
            "迭代 [1/1], 步骤 [85/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [86/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.18571430444717407\n",
            "迭代 [1/1], 步骤 [87/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.08571429550647736\n",
            "迭代 [1/1], 步骤 [88/500], GRPO更新 [1/1], 损失: -0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.44285717606544495\n",
            "迭代 [1/1], 步骤 [89/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.37142860889434814\n",
            "迭代 [1/1], 步骤 [90/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2785714566707611\n",
            "迭代 [1/1], 步骤 [91/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22857145965099335\n",
            "迭代 [1/1], 步骤 [92/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [93/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40714290738105774\n",
            "迭代 [1/1], 步骤 [94/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3571428656578064\n",
            "迭代 [1/1], 步骤 [95/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40000003576278687\n",
            "迭代 [1/1], 步骤 [96/500], GRPO更新 [1/1], 损失: -0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.25\n",
            "迭代 [1/1], 步骤 [97/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.19285717606544495\n",
            "迭代 [1/1], 步骤 [98/500], GRPO更新 [1/1], 损失: -0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [99/500], GRPO更新 [1/1], 损失: -0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7285715341567993\n",
            "迭代 [1/1], 步骤 [100/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [101/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0714285746216774\n",
            "迭代 [1/1], 步骤 [102/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [103/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5142857432365417\n",
            "迭代 [1/1], 步骤 [104/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3428571820259094\n",
            "迭代 [1/1], 步骤 [105/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [106/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0714285746216774\n",
            "迭代 [1/1], 步骤 [107/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3428571820259094\n",
            "迭代 [1/1], 步骤 [108/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.10000000149011612\n",
            "迭代 [1/1], 步骤 [109/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.392857164144516\n",
            "迭代 [1/1], 步骤 [110/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [111/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.44285714626312256\n",
            "迭代 [1/1], 步骤 [112/500], GRPO更新 [1/1], 损失: -0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.08571429550647736\n",
            "迭代 [1/1], 步骤 [113/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.26428574323654175\n",
            "迭代 [1/1], 步骤 [114/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.10000000894069672\n",
            "迭代 [1/1], 步骤 [115/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1785714328289032\n",
            "迭代 [1/1], 步骤 [116/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2071428745985031\n",
            "迭代 [1/1], 步骤 [117/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0714285746216774\n",
            "迭代 [1/1], 步骤 [118/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5214285850524902\n",
            "迭代 [1/1], 步骤 [119/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5071429014205933\n",
            "迭代 [1/1], 步骤 [120/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [121/500], GRPO更新 [1/1], 损失: -0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [122/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7071429491043091\n",
            "迭代 [1/1], 步骤 [123/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7285714745521545\n",
            "迭代 [1/1], 步骤 [124/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.43571433424949646\n",
            "迭代 [1/1], 步骤 [125/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40000003576278687\n",
            "迭代 [1/1], 步骤 [126/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5142857432365417\n",
            "迭代 [1/1], 步骤 [127/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [128/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.27142858505249023\n",
            "迭代 [1/1], 步骤 [129/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [130/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5571429133415222\n",
            "迭代 [1/1], 步骤 [131/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [132/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2142857313156128\n",
            "迭代 [1/1], 步骤 [133/500], GRPO更新 [1/1], 损失: -0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.30714288353919983\n",
            "迭代 [1/1], 步骤 [134/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6357143521308899\n",
            "迭代 [1/1], 步骤 [135/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [136/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.721428632736206\n",
            "迭代 [1/1], 步骤 [137/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.771428644657135\n",
            "迭代 [1/1], 步骤 [138/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3214285969734192\n",
            "迭代 [1/1], 步骤 [139/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1714285910129547\n",
            "迭代 [1/1], 步骤 [140/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.44285717606544495\n",
            "迭代 [1/1], 步骤 [141/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [142/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.07857143133878708\n",
            "迭代 [1/1], 步骤 [143/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.23571431636810303\n",
            "迭代 [1/1], 步骤 [144/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22857144474983215\n",
            "迭代 [1/1], 步骤 [145/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5214285850524902\n",
            "迭代 [1/1], 步骤 [146/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1785714328289032\n",
            "迭代 [1/1], 步骤 [147/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.10000000894069672\n",
            "迭代 [1/1], 步骤 [148/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.699999988079071\n",
            "迭代 [1/1], 步骤 [149/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0714285746216774\n",
            "迭代 [1/1], 步骤 [150/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.47857144474983215\n",
            "迭代 [1/1], 步骤 [151/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.19285716116428375\n",
            "迭代 [1/1], 步骤 [152/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.37142860889434814\n",
            "迭代 [1/1], 步骤 [153/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4214285910129547\n",
            "迭代 [1/1], 步骤 [154/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.13571429252624512\n",
            "迭代 [1/1], 步骤 [155/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0714285746216774\n",
            "迭代 [1/1], 步骤 [156/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.07857143133878708\n",
            "迭代 [1/1], 步骤 [157/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40714290738105774\n",
            "迭代 [1/1], 步骤 [158/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.09285715222358704\n",
            "迭代 [1/1], 步骤 [159/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.392857164144516\n",
            "迭代 [1/1], 步骤 [160/500], GRPO更新 [1/1], 损失: -0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1071429252624512\n",
            "迭代 [1/1], 步骤 [161/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22142857313156128\n",
            "迭代 [1/1], 步骤 [162/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [163/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.23571431636810303\n",
            "迭代 [1/1], 步骤 [164/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.19285716116428375\n",
            "迭代 [1/1], 步骤 [165/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [166/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5071429014205933\n",
            "迭代 [1/1], 步骤 [167/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.30000004172325134\n",
            "迭代 [1/1], 步骤 [168/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [169/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [170/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6857143640518188\n",
            "迭代 [1/1], 步骤 [171/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [172/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [173/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [174/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.13571429252624512\n",
            "迭代 [1/1], 步骤 [175/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.27142858505249023\n",
            "迭代 [1/1], 步骤 [176/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4142857491970062\n",
            "迭代 [1/1], 步骤 [177/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [178/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [179/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.07857143133878708\n",
            "迭代 [1/1], 步骤 [180/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2500000298023224\n",
            "迭代 [1/1], 步骤 [181/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6428571939468384\n",
            "迭代 [1/1], 步骤 [182/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.44285717606544495\n",
            "迭代 [1/1], 步骤 [183/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8785715103149414\n",
            "迭代 [1/1], 步骤 [184/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [185/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3500000238418579\n",
            "迭代 [1/1], 步骤 [186/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [187/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22142858803272247\n",
            "迭代 [1/1], 步骤 [188/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3142857253551483\n",
            "迭代 [1/1], 步骤 [189/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3428571820259094\n",
            "迭代 [1/1], 步骤 [190/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [191/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [192/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7071429491043091\n",
            "迭代 [1/1], 步骤 [193/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3214285969734192\n",
            "迭代 [1/1], 步骤 [194/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.857142984867096\n",
            "迭代 [1/1], 步骤 [195/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3571428656578064\n",
            "迭代 [1/1], 步骤 [196/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3857143521308899\n",
            "迭代 [1/1], 步骤 [197/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.47142863273620605\n",
            "迭代 [1/1], 步骤 [198/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40714290738105774\n",
            "迭代 [1/1], 步骤 [199/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.15714286267757416\n",
            "迭代 [1/1], 步骤 [200/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3857143223285675\n",
            "迭代 [1/1], 步骤 [201/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2428571581840515\n",
            "迭代 [1/1], 步骤 [202/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.771428644657135\n",
            "迭代 [1/1], 步骤 [203/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [204/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [205/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6857143640518188\n",
            "迭代 [1/1], 步骤 [206/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.32857146859169006\n",
            "迭代 [1/1], 步骤 [207/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5071429014205933\n",
            "迭代 [1/1], 步骤 [208/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3571428656578064\n",
            "迭代 [1/1], 步骤 [209/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.535714328289032\n",
            "迭代 [1/1], 步骤 [210/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.18571430444717407\n",
            "迭代 [1/1], 步骤 [211/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.11428572237491608\n",
            "迭代 [1/1], 步骤 [212/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [213/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.18571430444717407\n",
            "迭代 [1/1], 步骤 [214/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3428571820259094\n",
            "迭代 [1/1], 步骤 [215/500], GRPO更新 [1/1], 损失: -0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4285714626312256\n",
            "迭代 [1/1], 步骤 [216/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [217/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2428571581840515\n",
            "迭代 [1/1], 步骤 [218/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.18571430444717407\n",
            "迭代 [1/1], 步骤 [219/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.10000000894069672\n",
            "迭代 [1/1], 步骤 [220/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.23571431636810303\n",
            "迭代 [1/1], 步骤 [221/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1428571492433548\n",
            "迭代 [1/1], 步骤 [222/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [223/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6428571939468384\n",
            "迭代 [1/1], 步骤 [224/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.2071430683135986\n",
            "迭代 [1/1], 步骤 [225/500], GRPO更新 [1/1], 损失: -0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4642857313156128\n",
            "迭代 [1/1], 步骤 [226/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [227/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.37142860889434814\n",
            "迭代 [1/1], 步骤 [228/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.11428572237491608\n",
            "迭代 [1/1], 步骤 [229/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [230/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.485714316368103\n",
            "迭代 [1/1], 步骤 [231/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.06428571790456772\n",
            "迭代 [1/1], 步骤 [232/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.48571428656578064\n",
            "迭代 [1/1], 步骤 [233/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40714284777641296\n",
            "迭代 [1/1], 步骤 [234/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5214285850524902\n",
            "迭代 [1/1], 步骤 [235/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0\n",
            "迭代 [1/1], 步骤 [236/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.08571429550647736\n",
            "迭代 [1/1], 步骤 [237/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.30000001192092896\n",
            "迭代 [1/1], 步骤 [238/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3857143223285675\n",
            "迭代 [1/1], 步骤 [239/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6428571939468384\n",
            "迭代 [1/1], 步骤 [240/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3928571939468384\n",
            "迭代 [1/1], 步骤 [241/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5285714864730835\n",
            "迭代 [1/1], 步骤 [242/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3428571820259094\n",
            "迭代 [1/1], 步骤 [243/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4928572177886963\n",
            "迭代 [1/1], 步骤 [244/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6642857789993286\n",
            "迭代 [1/1], 步骤 [245/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6714285612106323\n",
            "迭代 [1/1], 步骤 [246/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3214285969734192\n",
            "迭代 [1/1], 步骤 [247/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.16428573429584503\n",
            "迭代 [1/1], 步骤 [248/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [249/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.30000004172325134\n",
            "迭代 [1/1], 步骤 [250/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [251/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1071429252624512\n",
            "迭代 [1/1], 步骤 [252/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [253/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [254/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0\n",
            "迭代 [1/1], 步骤 [255/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.13571429252624512\n",
            "迭代 [1/1], 步骤 [256/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.36428576707839966\n",
            "迭代 [1/1], 步骤 [257/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7000000476837158\n",
            "迭代 [1/1], 步骤 [258/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8928572535514832\n",
            "迭代 [1/1], 步骤 [259/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.19285713136196136\n",
            "迭代 [1/1], 步骤 [260/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0714285746216774\n",
            "迭代 [1/1], 步骤 [261/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3142857253551483\n",
            "迭代 [1/1], 步骤 [262/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5214285850524902\n",
            "迭代 [1/1], 步骤 [263/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8571429252624512\n",
            "迭代 [1/1], 步骤 [264/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.535714328289032\n",
            "迭代 [1/1], 步骤 [265/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6285714507102966\n",
            "迭代 [1/1], 步骤 [266/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8571429252624512\n",
            "迭代 [1/1], 步骤 [267/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1714285910129547\n",
            "迭代 [1/1], 步骤 [268/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.32857146859169006\n",
            "迭代 [1/1], 步骤 [269/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0\n",
            "迭代 [1/1], 步骤 [270/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1428571492433548\n",
            "迭代 [1/1], 步骤 [271/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0357142873108387\n",
            "迭代 [1/1], 步骤 [272/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6428572535514832\n",
            "迭代 [1/1], 步骤 [273/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.442857265472412\n",
            "迭代 [1/1], 步骤 [274/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40714284777641296\n",
            "迭代 [1/1], 步骤 [275/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2857142984867096\n",
            "迭代 [1/1], 步骤 [276/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5285714864730835\n",
            "迭代 [1/1], 步骤 [277/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.09285714477300644\n",
            "迭代 [1/1], 步骤 [278/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.735714316368103\n",
            "迭代 [1/1], 步骤 [279/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2142857313156128\n",
            "迭代 [1/1], 步骤 [280/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22857145965099335\n",
            "迭代 [1/1], 步骤 [281/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.37142860889434814\n",
            "迭代 [1/1], 步骤 [282/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0357143878936768\n",
            "迭代 [1/1], 步骤 [283/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9142857789993286\n",
            "迭代 [1/1], 步骤 [284/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.857142984867096\n",
            "迭代 [1/1], 步骤 [285/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.721428632736206\n",
            "迭代 [1/1], 步骤 [286/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4928571283817291\n",
            "迭代 [1/1], 步骤 [287/500], GRPO更新 [1/1], 损失: -0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8714286684989929\n",
            "迭代 [1/1], 步骤 [288/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.07857143133878708\n",
            "迭代 [1/1], 步骤 [289/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3500001430511475\n",
            "迭代 [1/1], 步骤 [290/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.735714316368103\n",
            "迭代 [1/1], 步骤 [291/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0357143878936768\n",
            "迭代 [1/1], 步骤 [292/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9285714626312256\n",
            "迭代 [1/1], 步骤 [293/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3928571939468384\n",
            "迭代 [1/1], 步骤 [294/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3500000536441803\n",
            "迭代 [1/1], 步骤 [295/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.23571431636810303\n",
            "迭代 [1/1], 步骤 [296/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.36428576707839966\n",
            "迭代 [1/1], 步骤 [297/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8214285969734192\n",
            "迭代 [1/1], 步骤 [298/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3071428537368774\n",
            "迭代 [1/1], 步骤 [299/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.06428571790456772\n",
            "迭代 [1/1], 步骤 [300/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0\n",
            "迭代 [1/1], 步骤 [301/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7071429491043091\n",
            "迭代 [1/1], 步骤 [302/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7785714864730835\n",
            "迭代 [1/1], 步骤 [303/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40714290738105774\n",
            "迭代 [1/1], 步骤 [304/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.821428656578064\n",
            "迭代 [1/1], 步骤 [305/500], GRPO更新 [1/1], 损失: 0.0029\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1785714626312256\n",
            "迭代 [1/1], 步骤 [306/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5428571701049805\n",
            "迭代 [1/1], 步骤 [307/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8071429133415222\n",
            "迭代 [1/1], 步骤 [308/500], GRPO更新 [1/1], 损失: 0.0016\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4928572177886963\n",
            "迭代 [1/1], 步骤 [309/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2857142984867096\n",
            "迭代 [1/1], 步骤 [310/500], GRPO更新 [1/1], 损失: -0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2428571581840515\n",
            "迭代 [1/1], 步骤 [311/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6642857789993286\n",
            "迭代 [1/1], 步骤 [312/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4214285910129547\n",
            "迭代 [1/1], 步骤 [313/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7357143759727478\n",
            "迭代 [1/1], 步骤 [314/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1714285910129547\n",
            "迭代 [1/1], 步骤 [315/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4571429193019867\n",
            "迭代 [1/1], 步骤 [316/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3000000715255737\n",
            "迭代 [1/1], 步骤 [317/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3285714387893677\n",
            "迭代 [1/1], 步骤 [318/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [319/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1071428656578064\n",
            "迭代 [1/1], 步骤 [320/500], GRPO更新 [1/1], 损失: 0.0019\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.02857143059372902\n",
            "迭代 [1/1], 步骤 [321/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0357143878936768\n",
            "迭代 [1/1], 步骤 [322/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5214285850524902\n",
            "迭代 [1/1], 步骤 [323/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3428571820259094\n",
            "迭代 [1/1], 步骤 [324/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.37142860889434814\n",
            "迭代 [1/1], 步骤 [325/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9642857313156128\n",
            "迭代 [1/1], 步骤 [326/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [327/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7500000596046448\n",
            "迭代 [1/1], 步骤 [328/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0357143878936768\n",
            "迭代 [1/1], 步骤 [329/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5500000715255737\n",
            "迭代 [1/1], 步骤 [330/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.36428573727607727\n",
            "迭代 [1/1], 步骤 [331/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6000000834465027\n",
            "迭代 [1/1], 步骤 [332/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9571429491043091\n",
            "迭代 [1/1], 步骤 [333/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4642857313156128\n",
            "迭代 [1/1], 步骤 [334/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8857142925262451\n",
            "迭代 [1/1], 步骤 [335/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.485714316368103\n",
            "迭代 [1/1], 步骤 [336/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5428571701049805\n",
            "迭代 [1/1], 步骤 [337/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9071429371833801\n",
            "迭代 [1/1], 步骤 [338/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4142857491970062\n",
            "迭代 [1/1], 步骤 [339/500], GRPO更新 [1/1], 损失: -0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3214285969734192\n",
            "迭代 [1/1], 步骤 [340/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0\n",
            "迭代 [1/1], 步骤 [341/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7357143759727478\n",
            "迭代 [1/1], 步骤 [342/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.2142857313156128\n",
            "迭代 [1/1], 步骤 [343/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22142858803272247\n",
            "迭代 [1/1], 步骤 [344/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3571428656578064\n",
            "迭代 [1/1], 步骤 [345/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6142857670783997\n",
            "迭代 [1/1], 步骤 [346/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.4357143640518188\n",
            "迭代 [1/1], 步骤 [347/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.214285746216774\n",
            "迭代 [1/1], 步骤 [348/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5928572416305542\n",
            "迭代 [1/1], 步骤 [349/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7571429014205933\n",
            "迭代 [1/1], 步骤 [350/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5214285850524902\n",
            "迭代 [1/1], 步骤 [351/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0142858028411865\n",
            "迭代 [1/1], 步骤 [352/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5714285969734192\n",
            "迭代 [1/1], 步骤 [353/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.0357142873108387\n",
            "迭代 [1/1], 步骤 [354/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.392857164144516\n",
            "迭代 [1/1], 步骤 [355/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.44285717606544495\n",
            "迭代 [1/1], 步骤 [356/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6285714507102966\n",
            "迭代 [1/1], 步骤 [357/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1785714626312256\n",
            "迭代 [1/1], 步骤 [358/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.071428656578064\n",
            "迭代 [1/1], 步骤 [359/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05000000447034836\n",
            "迭代 [1/1], 步骤 [360/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6428571939468384\n",
            "迭代 [1/1], 步骤 [361/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0285714864730835\n",
            "迭代 [1/1], 步骤 [362/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6285713911056519\n",
            "迭代 [1/1], 步骤 [363/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1785714328289032\n",
            "迭代 [1/1], 步骤 [364/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3500001430511475\n",
            "迭代 [1/1], 步骤 [365/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5142857432365417\n",
            "迭代 [1/1], 步骤 [366/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5571429133415222\n",
            "迭代 [1/1], 步骤 [367/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5214285850524902\n",
            "迭代 [1/1], 步骤 [368/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6857143640518188\n",
            "迭代 [1/1], 步骤 [369/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [370/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1142858266830444\n",
            "迭代 [1/1], 步骤 [371/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0214287042617798\n",
            "迭代 [1/1], 步骤 [372/500], GRPO更新 [1/1], 损失: -0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [373/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1357144117355347\n",
            "迭代 [1/1], 步骤 [374/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7785714864730835\n",
            "迭代 [1/1], 步骤 [375/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7571429014205933\n",
            "迭代 [1/1], 步骤 [376/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8142858147621155\n",
            "迭代 [1/1], 步骤 [377/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8928571939468384\n",
            "迭代 [1/1], 步骤 [378/500], GRPO更新 [1/1], 损失: -0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.43571433424949646\n",
            "迭代 [1/1], 步骤 [379/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8071429133415222\n",
            "迭代 [1/1], 步骤 [380/500], GRPO更新 [1/1], 损失: 0.0016\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.37142857909202576\n",
            "迭代 [1/1], 步骤 [381/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5928571820259094\n",
            "迭代 [1/1], 步骤 [382/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7142857313156128\n",
            "迭代 [1/1], 步骤 [383/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.26428574323654175\n",
            "迭代 [1/1], 步骤 [384/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.1785714328289032\n",
            "迭代 [1/1], 步骤 [385/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9500001072883606\n",
            "迭代 [1/1], 步骤 [386/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6714286208152771\n",
            "迭代 [1/1], 步骤 [387/500], GRPO更新 [1/1], 损失: -0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4214285910129547\n",
            "迭代 [1/1], 步骤 [388/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6714286208152771\n",
            "迭代 [1/1], 步骤 [389/500], GRPO更新 [1/1], 损失: -0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.2571428716182709\n",
            "迭代 [1/1], 步骤 [390/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4928571581840515\n",
            "迭代 [1/1], 步骤 [391/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3571428954601288\n",
            "迭代 [1/1], 步骤 [392/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.5000001192092896\n",
            "迭代 [1/1], 步骤 [393/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.485714316368103\n",
            "迭代 [1/1], 步骤 [394/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9500001072883606\n",
            "迭代 [1/1], 步骤 [395/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.485714316368103\n",
            "迭代 [1/1], 步骤 [396/500], GRPO更新 [1/1], 损失: -0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1785715818405151\n",
            "迭代 [1/1], 步骤 [397/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.6142858266830444\n",
            "迭代 [1/1], 步骤 [398/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.4642857313156128\n",
            "迭代 [1/1], 步骤 [399/500], GRPO更新 [1/1], 损失: 0.0016\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.5071429014205933\n",
            "迭代 [1/1], 步骤 [400/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40000003576278687\n",
            "迭代 [1/1], 步骤 [401/500], GRPO更新 [1/1], 损失: -0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8928571939468384\n",
            "迭代 [1/1], 步骤 [402/500], GRPO更新 [1/1], 损失: 0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.32857146859169006\n",
            "迭代 [1/1], 步骤 [403/500], GRPO更新 [1/1], 损失: -0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0785715579986572\n",
            "迭代 [1/1], 步骤 [404/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.735714316368103\n",
            "迭代 [1/1], 步骤 [405/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.2142857313156128\n",
            "迭代 [1/1], 步骤 [406/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.164285659790039\n",
            "迭代 [1/1], 步骤 [407/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5500000715255737\n",
            "迭代 [1/1], 步骤 [408/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9357143640518188\n",
            "迭代 [1/1], 步骤 [409/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22142860293388367\n",
            "迭代 [1/1], 步骤 [410/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8285714983940125\n",
            "迭代 [1/1], 步骤 [411/500], GRPO更新 [1/1], 损失: -0.0000\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5\n",
            "迭代 [1/1], 步骤 [412/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7500000596046448\n",
            "迭代 [1/1], 步骤 [413/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3857143223285675\n",
            "迭代 [1/1], 步骤 [414/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4642857313156128\n",
            "迭代 [1/1], 步骤 [415/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9285714626312256\n",
            "迭代 [1/1], 步骤 [416/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7642858028411865\n",
            "迭代 [1/1], 步骤 [417/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7357142567634583\n",
            "迭代 [1/1], 步骤 [418/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.30714288353919983\n",
            "迭代 [1/1], 步骤 [419/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22857145965099335\n",
            "迭代 [1/1], 步骤 [420/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.7642858028411865\n",
            "迭代 [1/1], 步骤 [421/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8071429133415222\n",
            "迭代 [1/1], 步骤 [422/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.2000000476837158\n",
            "迭代 [1/1], 步骤 [423/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.771428644657135\n",
            "迭代 [1/1], 步骤 [424/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.2428572177886963\n",
            "迭代 [1/1], 步骤 [425/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9285715222358704\n",
            "迭代 [1/1], 步骤 [426/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.5785715579986572\n",
            "迭代 [1/1], 步骤 [427/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9857142567634583\n",
            "迭代 [1/1], 步骤 [428/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1642858982086182\n",
            "迭代 [1/1], 步骤 [429/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8785715103149414\n",
            "迭代 [1/1], 步骤 [430/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.8428571224212646\n",
            "迭代 [1/1], 步骤 [431/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.2571429014205933\n",
            "迭代 [1/1], 步骤 [432/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9000000953674316\n",
            "迭代 [1/1], 步骤 [433/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9285714626312256\n",
            "迭代 [1/1], 步骤 [434/500], GRPO更新 [1/1], 损失: -0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.47857144474983215\n",
            "迭代 [1/1], 步骤 [435/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7000001072883606\n",
            "迭代 [1/1], 步骤 [436/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0928571224212646\n",
            "迭代 [1/1], 步骤 [437/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3928571939468384\n",
            "迭代 [1/1], 步骤 [438/500], GRPO更新 [1/1], 损失: 0.0017\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4214285910129547\n",
            "迭代 [1/1], 步骤 [439/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.2357144355773926\n",
            "迭代 [1/1], 步骤 [440/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6142857670783997\n",
            "迭代 [1/1], 步骤 [441/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3142857551574707\n",
            "迭代 [1/1], 步骤 [442/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1714285612106323\n",
            "迭代 [1/1], 步骤 [443/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.7428571581840515\n",
            "迭代 [1/1], 步骤 [444/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.192857265472412\n",
            "迭代 [1/1], 步骤 [445/500], GRPO更新 [1/1], 损失: 0.0014\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8857144117355347\n",
            "迭代 [1/1], 步骤 [446/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.26428574323654175\n",
            "迭代 [1/1], 步骤 [447/500], GRPO更新 [1/1], 损失: 0.0012\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5285714864730835\n",
            "迭代 [1/1], 步骤 [448/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.37857145071029663\n",
            "迭代 [1/1], 步骤 [449/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3142858743667603\n",
            "迭代 [1/1], 步骤 [450/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.15714286267757416\n",
            "迭代 [1/1], 步骤 [451/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6214286088943481\n",
            "迭代 [1/1], 步骤 [452/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [453/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5714285969734192\n",
            "迭代 [1/1], 步骤 [454/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6857143640518188\n",
            "迭代 [1/1], 步骤 [455/500], GRPO更新 [1/1], 损失: 0.0047\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.5214287042617798\n",
            "迭代 [1/1], 步骤 [456/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6928571462631226\n",
            "迭代 [1/1], 步骤 [457/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.15000002086162567\n",
            "迭代 [1/1], 步骤 [458/500], GRPO更新 [1/1], 损失: 0.0003\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9500001072883606\n",
            "迭代 [1/1], 步骤 [459/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.9428571462631226\n",
            "迭代 [1/1], 步骤 [460/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8000000715255737\n",
            "迭代 [1/1], 步骤 [461/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.785714328289032\n",
            "迭代 [1/1], 步骤 [462/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.2428573369979858\n",
            "迭代 [1/1], 步骤 [463/500], GRPO更新 [1/1], 损失: 0.0017\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.9428571462631226\n",
            "迭代 [1/1], 步骤 [464/500], GRPO更新 [1/1], 损失: 0.0015\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4928571581840515\n",
            "迭代 [1/1], 步骤 [465/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0500000715255737\n",
            "迭代 [1/1], 步骤 [466/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.22142858803272247\n",
            "迭代 [1/1], 步骤 [467/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.4857144355773926\n",
            "迭代 [1/1], 步骤 [468/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.8357143998146057\n",
            "迭代 [1/1], 步骤 [469/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1000001430511475\n",
            "迭代 [1/1], 步骤 [470/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.11428572237491608\n",
            "迭代 [1/1], 步骤 [471/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.571428656578064\n",
            "迭代 [1/1], 步骤 [472/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3500001430511475\n",
            "迭代 [1/1], 步骤 [473/500], GRPO更新 [1/1], 损失: 0.0018\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.08571429550647736\n",
            "迭代 [1/1], 步骤 [474/500], GRPO更新 [1/1], 损失: 0.0001\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.01428571529686451\n",
            "迭代 [1/1], 步骤 [475/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.6071428656578064\n",
            "迭代 [1/1], 步骤 [476/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1428571939468384\n",
            "迭代 [1/1], 步骤 [477/500], GRPO更新 [1/1], 损失: 0.0017\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.04285714775323868\n",
            "迭代 [1/1], 步骤 [478/500], GRPO更新 [1/1], 损失: 0.0011\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3928571939468384\n",
            "迭代 [1/1], 步骤 [479/500], GRPO更新 [1/1], 损失: 0.0019\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.1428571939468384\n",
            "迭代 [1/1], 步骤 [480/500], GRPO更新 [1/1], 损失: 0.0004\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.550000011920929\n",
            "迭代 [1/1], 步骤 [481/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.4928573369979858\n",
            "迭代 [1/1], 步骤 [482/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.4928572177886963\n",
            "迭代 [1/1], 步骤 [483/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.5642857551574707\n",
            "迭代 [1/1], 步骤 [484/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.3857144117355347\n",
            "迭代 [1/1], 步骤 [485/500], GRPO更新 [1/1], 损失: 0.0006\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.05714286118745804\n",
            "迭代 [1/1], 步骤 [486/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3142857253551483\n",
            "迭代 [1/1], 步骤 [487/500], GRPO更新 [1/1], 损失: 0.0013\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.4571428894996643\n",
            "迭代 [1/1], 步骤 [488/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.5785715579986572\n",
            "迭代 [1/1], 步骤 [489/500], GRPO更新 [1/1], 损失: 0.0009\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.20000001788139343\n",
            "迭代 [1/1], 步骤 [490/500], GRPO更新 [1/1], 损失: 0.0002\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.0785715579986572\n",
            "迭代 [1/1], 步骤 [491/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.12857143580913544\n",
            "迭代 [1/1], 步骤 [492/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.5357143878936768\n",
            "迭代 [1/1], 步骤 [493/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.4142857789993286\n",
            "迭代 [1/1], 步骤 [494/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.79285728931427\n",
            "迭代 [1/1], 步骤 [495/500], GRPO更新 [1/1], 损失: 0.0008\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.3571428656578064\n",
            "迭代 [1/1], 步骤 [496/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.40000003576278687\n",
            "迭代 [1/1], 步骤 [497/500], GRPO更新 [1/1], 损失: 0.0010\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.157142996788025\n",
            "迭代 [1/1], 步骤 [498/500], GRPO更新 [1/1], 损失: 0.0007\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 1.721428632736206\n",
            "迭代 [1/1], 步骤 [499/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Output batch size: 14, Device after model: cuda:0\n",
            "Average Reward: 0.30000001192092896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "迭代 [1/1], 步骤 [500/500], GRPO更新 [1/1], 损失: 0.0005\n",
            "Training completed and wandb run finished.\n",
            "\n",
            "Final model evaluation after GRPO RL fine-tuning:\n",
            "\n",
            "==================================================\n",
            "EVALUATION ON 30 EXAMPLES\n",
            "==================================================\n",
            "\n",
            "Accuracy: 23.33% (7/30)\n",
            "==================================================\n",
            "Post-GRPO Accuracy: 23.33%\n",
            "\n",
            "Saving GRPO fine-tuned model...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('dlc_1.5B_model/tokenizer_config.json',\n",
              " 'dlc_1.5B_model/special_tokens_map.json',\n",
              " 'dlc_1.5B_model/vocab.json',\n",
              " 'dlc_1.5B_model/merges.txt',\n",
              " 'dlc_1.5B_model/added_tokens.json',\n",
              " 'dlc_1.5B_model/tokenizer.json')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def optimize_model_memory(model):\n",
        "    \"\"\"\n",
        "    Optimizes the model to use less memory during training.\n",
        "\n",
        "    Args:\n",
        "        model: The language model to optimize.\n",
        "\n",
        "    Returns:\n",
        "        The optimized model.\n",
        "\n",
        "    Explanation:\n",
        "        1. Sets the model to training mode.\n",
        "        2. Disables KV caching to save memory.\n",
        "        3. Enables gradient checkpointing to trade computation for memory.\n",
        "        4. Ensures that input embeddings require gradients:\n",
        "           - Either uses the built-in method if available.\n",
        "           - Or adds a forward hook to the input embeddings layer.\n",
        "        5. Returns the optimized model ready for memory-efficient training.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    # First ensure inputs will require gradients\n",
        "    if hasattr(model, \"enable_input_require_grads\"):\n",
        "        model.enable_input_require_grads()\n",
        "    else:\n",
        "        def make_inputs_require_grad(module, input, output):\n",
        "            output.requires_grad_(True)\n",
        "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
        "\n",
        "    # Then enable gradient checkpointing\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Main execution\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using primary device: {device}\")\n",
        "\n",
        "# 获取及下载模型\n",
        "# modelscope download --model Qwen/Qwen2.5-7B-Instruct --local_dir /home/yeqi3/cyr/code/Basic-LLM-Learning/Code/LLM05/models/Qwen2.5-7B-Instruct\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "output_dir = \"math_solver_model\"\n",
        "\n",
        "print(\"Downloading model...\")\n",
        "# 这里用的是将模型下载到本地之后再指定模型路径进行读取\n",
        "base_model_path = r\"./models/Qwen2.5-1.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=base_model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False\n",
        ")\n",
        "print(\"Model downloaded\")\n",
        "\n",
        "# 获取及下载分词器\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=base_model_path,\n",
        "    padding_side=\"left\",\n",
        "    use_cache=False\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "model.config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# 获取显卡数量\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Detected {num_gpus} GPUs\")\n",
        "device_ids = list(range(num_gpus)) if num_gpus > 1 else None\n",
        "\n",
        "model = optimize_model_memory(model)\n",
        "\n",
        "print(\"\\nStarting RL fine-tuning using GRPO...\")\n",
        "training_config = {\n",
        "    'num_iterations': 1,\n",
        "    'num_steps': 500,\n",
        "    'batch_size': 1,                # reduce if you have fewer GPUs\n",
        "    'num_generations': 14,          # reduce if you have GPUs with less VRAM\n",
        "    'max_completion_length': 400,   # reduce if you have GPUs with less VRAM\n",
        "    'beta': 0.04,\n",
        "    'learning_rate': 5e-6,\n",
        "    'mu': 1,\n",
        "    'epsilon': 0.1\n",
        "}\n",
        "\n",
        "print(\"Train Config:\")\n",
        "print(training_config)\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "print(\"Weights & Biases initialized.\")\n",
        "\n",
        "model = train_with_grpo(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_data=train_data,\n",
        "    reward_function=combined_reward,\n",
        "    device_ids=device_ids,\n",
        "    **training_config\n",
        ")\n",
        "\n",
        "print(\"Training completed and wandb run finished.\")\n",
        "\n",
        "print(\"\\nFinal model evaluation after GRPO RL fine-tuning:\")\n",
        "post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
        "print(f\"Post-GRPO Accuracy: {post_grpo_accuracy:.2f}%\")\n",
        "\n",
        "print(\"\\nSaving GRPO fine-tuned model...\")\n",
        "model.save_pretrained(\"dlc_1.5B_model\")\n",
        "tokenizer.save_pretrained(\"dlc_1.5B_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7541a038",
      "metadata": {},
      "source": [
        "# 第八部分：对比模型效果\n",
        "\n",
        "使用原始模型和GRPO训练后的模型进行训练，统一调用的是本代码中实现的`evaluate_model`函数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "363428c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading original model for evaluation...\n",
            "\n",
            "Loading fine-tuned model for evaluation...\n",
            "\n",
            "Evaluating original model...\n",
            "\n",
            "==================================================\n",
            "EVALUATION ON 30 EXAMPLES\n",
            "==================================================\n",
            "\n",
            "Accuracy: 3.33% (1/30)\n",
            "==================================================\n",
            "\n",
            "Evaluating fine-tuned model...\n",
            "\n",
            "==================================================\n",
            "EVALUATION ON 30 EXAMPLES\n",
            "==================================================\n",
            "\n",
            "Accuracy: 60.00% (18/30)\n",
            "==================================================\n",
            "Original Model Accuracy: 3.33%\n",
            "Fine-tuned Model Accuracy: 60.00%\n"
          ]
        }
      ],
      "source": [
        "# 加载原始模型（未优化的原始模型）\n",
        "print(\"\\nLoading original model for evaluation...\")\n",
        "original_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=base_model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False  # 评估时启用缓存加速生成\n",
        ")\n",
        "original_model.eval()  # 设置为评估模式\n",
        "\n",
        "# 加载微调后的模型\n",
        "print(\"\\nLoading fine-tuned model for evaluation...\")\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"dlc_1.5B_model\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False  # 评估时启用缓存加速生成\n",
        ")\n",
        "finetuned_model.eval()  # 设置为评估模式\n",
        "\n",
        "# 确保分词器设置一致\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=base_model_path,\n",
        "    padding_side=\"left\"\n",
        ")\n",
        "original_tokenizer.pad_token = original_tokenizer.eos_token\n",
        "\n",
        "# 评估原始模型\n",
        "print(\"\\nEvaluating original model...\")\n",
        "with torch.no_grad():\n",
        "    original_accuracy = evaluate_model(\n",
        "        model=original_model,\n",
        "        tokenizer=original_tokenizer,\n",
        "        eval_examples=eval_data,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "# 评估微调后的模型\n",
        "print(\"\\nEvaluating fine-tuned model...\")\n",
        "with torch.no_grad():\n",
        "    finetuned_accuracy = evaluate_model(\n",
        "        model=finetuned_model,\n",
        "        tokenizer=tokenizer,  # 使用训练时保存的分词器\n",
        "        eval_examples=eval_data,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "print(f\"Original Model Accuracy: {original_accuracy:.2f}%\")\n",
        "print(f\"Fine-tuned Model Accuracy: {finetuned_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qgaQzigaSYxu",
      "metadata": {
        "id": "qgaQzigaSYxu"
      },
      "source": [
        "正如你所看到的，模型没有学会生成序列结束（`EOS`）标记，因此输出序列在 `</answer>` 标签之后仍然继续。这是预期的行为，因为我们使用的奖励函数没有为停止生成提供特殊的奖励。我们也没有进行监督微调步骤（如我们[在这里](https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb)所做的那样），在那里我们可以教会模型在 `</answer>` 之后生成 `EOS`。\n",
        "\n",
        "这就是本教程的结束。此时，你应该对GRPO算法的工作原理以及构建用于微调语言模型以完成数学、编码和逻辑任务的RL管道所需的组件有了清晰的理解。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SvNrAkktTv4b",
      "metadata": {
        "id": "SvNrAkktTv4b"
      },
      "source": [
        "<div style=\"display: flex; justify-content: center;\">\n",
        "    <div style=\"background-color: #f4f6f7; padding: 15px; width: 80%;\">\n",
        "        <table style=\"width: 100%\">\n",
        "            <tr>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <span style=\"font-size: 14px;\">\n",
        "                        This was an extension notebook for <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">The Hundred-Page Language Models Book</a> by Andriy Burkov<br><br>\n",
        "                        Code repository: <a href=\"https://github.com/aburkov/theLMbook\" target=\"_blank\" rel=\"noopener\">https://github.com/aburkov/theLMbook</a>\n",
        "                        <br><br>\n",
        "                        <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">Read the book</a> to learn about language modeling and train yours from scratch.\n",
        "                    </span>\n",
        "                </td>\n",
        "                <td style=\"vertical-align: middle;\">\n",
        "                    <a href=\"https://www.thelmbook.com\" target=\"_blank\" rel=\"noopener\">\n",
        "                        <img src=\"https://thelmbook.com/img/book.png\" width=\"80px\" alt=\"The Hundred-Page Language Models Book\">\n",
        "                    </a>\n",
        "                </td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </div>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cyr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
